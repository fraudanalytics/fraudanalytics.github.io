## Workshop Info and Materials

<details open>
  <summary><h3><strong>PiML Training, March 21-22, 2024 | Commonwealth Bank </strong></h3></summary><br /> 

**Speaker:** Vijay Nair and Aijun Zhang

**Slides:** 

- [Session 1: Machine Learning - Conceptual Soundness](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202403CBA/202403CBA_ConceptualSoundness.pdf)
- [Session 2: Machine Learning - Outcome Analysis](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202403CBA/202403CBA_OutcomeAnalysis.pdf)

**Codes:** 
- <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202403CBA/202403PiML_Interpretability_SimuCredit.ipynb">PiML Model Interpretability (SimuCredit Example)</a>  
- <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202403CBA/202403PiML_OutcomeAnalysis_SimuCredit.ipynb">PiML Outcome Analysis (SimuCredit Example)</a>  
</details>  



<details open>
  <summary><h3><strong>AI/ML Model Interpretability and Outcome Analysis, Feb, 2024 | TD Bank</strong></h3></summary><br /> 

**Speaker:** Aijun Zhang

**Slides:** 

- [Session 1: AI/ML Model Interpretability](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202402TDBank/202402PiML_TD_Session1.pdf) 
- [Session 2: AI/ML Outcome Analysis](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202402TDBank/202402PiML_TD_Session2.pdf) 

**Codes:** 
- <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202402TDBank/202402PiML-Interpretability-SimuCredit.ipynb">PiML Model Interpretability (SimuCredit Example)</a>  
- <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202402TDBank/202402PiML-OutcomeAnalysis-SimuCredit.ipynb">PiML Outcome Analysis (SimuCredit Example)</a>  
</details>  



<details open>
  <summary><h3><strong>Machine Learning Model Validation, Dec 4, 2023 & Jan XX,  2024 | GM Financial</strong></h3></summary><br /> 

**Speaker:** Aijun Zhang

**Slides:** 

- [Session 1: Model Interpretability](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202311PiML_GMFinancial_Session1.pdf) 
- Session 2: Outcome Testing

**Codes:** 
<a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202312PiML-SimuCredit Example.ipynb">SimuCredit Example using PiML</a>  
</details>  


<details open>
  <summary><h3><strong>Model Diagnostics Lecture Materials | Model Validation Class, UNCC Master of Data Science, Fall 2023</strong></h3></summary><br /> 

Instructor: Aijun Zhang

Click the ipynb links to run notebooks through Google Colab:

**Part 1: Error and Resilience** (<a href="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202310-UNCC/202310ModelDiagTrilogyPart1.pdf">PDF</a>)
 
- CaliforniaHousing Case (Regression) <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202310-UNCC/ModelDiagPart1_Error-n-Resilience_CaliforniaHousing.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  
- SimuCredit Case (Binary Classification) <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202310-UNCC/ModelDiagPart1_Error-n-Resilience_SimuCredit.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>

**Part 2: Prediction Uncertainty** (<a href="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202310-UNCC/202310ModelDiagnosticsPart2.pdf">PDF</a>)

- CaliforniaHousing Case (Regression) <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202310-UNCC/ModelDiagPart2_PredictionUncertainty_CaliforniaHousing.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  
- SimuCredit Case (Binary Classification) <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202310-UNCC/ModelDiagPart2_PredictionUncertainty_SimuCredit.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>
</details>  


<details open>
  <summary><h3><strong>Workshop at GFMI 2nd Edition Credit Risk Modeling, September 6-7, 2023 | Chicago</strong></h3></summary><br /> 

**Speaker:** Aijun Zhang

**Slides:** [Develop Enhanced Credit Risk Models through Interpretable Machine Learning](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202309CreditRisk.pdf) 

**Codes:** <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202309PiML-CreditRisk.ipynb">Hands-on PiML Tutorial with SimuCredit Data</a>  
</details>  

<details open>
  <summary><h3><strong>Responsible Machine Learning, July 2023 | Quant Training</strong></h3></summary><br /> 

  **Speaker:** Aijun Zhang

20230713ResML_BikeSharing: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202307-ResML/20230713ResML_BikeSharing.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  

20230713ResML_ReLU-DNN_CoCircles: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202307-ResML/20230713ResML_ReLU-DNN_CoCircles.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  

20230713ResML_ReLU-DNN_TaiwanCredit: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202307-ResML/20230713ResML_ReLU-DNN_TaiwanCredit.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  

20230713ResML_CaliforniaHousing: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202307-ResML/20230713ResML_CaliforniaHousing.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  

20230713ResML_Model_Fairness: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202307-ResML/20230713ResML_Model_Fairness.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  

</details>  

<details open>
  <summary><h3><strong>Machine Learning Model Validation Workshop, April 18, 2023 | Truist</strong></h3></summary><br /> 
  
**Speakers:** Agus Sudjianto, Vijay Nair and Anwesha Bhattacharyya

The focus of this workshop is to provide the latest development in model validation for Machine Learning with special emphasis on evaluation of conceptual soundness and outcome analysis. Key topics for conceptual soundness includes model causality, explainability and interpretability, as well as dealing with over-parameterized/under-specification problems commonly occurred in machine learning models. Designing inherently interpretable machine learning models will be discussed in-depth considering the importance of the methodology for high-risk applications as well as its role for model benchmark.


**Example Notebooks:** (provided by Anwesha) 

  - <a href="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/Truist_Credit_Analysis.ipynb">Credit_Analysis.ipynb</a>
  
</details>  


<br /> 


<details open>
  <summary><h3><strong>Quality AI/ML Models by Design With PiML, January 20, 2023 | OpenTeams</strong></h3></summary><br /> 
  
**Speakers:** Agus Sudjianto

Machine Learning (ML) is quickly becoming ubiquitous across many sectors, including in banking for both predictive analytics and process automation applications. As a highly regulated industry, banks must be extremely cautious in adopting ML in high-risk areas such as credit underwriting. Accordingly, they are at the forefront of method and tooling development for creation and implementation of high-quality ML models with strong reliability, fairness, explainability, and resiliency characteristics. A conceptually sound ML model must make minimal tradeoffs between all of these characteristics—without continuous retraining—and exhibit robustness in ever-changing environments.

In this OpenTeams community event, members of the PiML team present the project and its capabilities, including specific discussion of PiML’s applications in evaluating ML model bias, and successful use of PiML in enterprise. Listen in to learn more about this exciting project that will help you build better ML models!

Video link: [https://www.youtube.com/watch?v=R79JGIMf6PY](https://www.youtube.com/playlist?list=PLA7hukZmf3Aogn4fx8qlTJrACW-lIk1tE)

</details>  


<br /> 

<details open>
  <summary><h3><strong>Machine Learning Model Diagnostics and Validation, July 22, 2022 </strong></h3></summary><br /> 
  
**Speakers:** Agus Sudjianto and Aijun Zhang

**Demo Notebooks:** click the ipynb links to run Jupyter notebooks in Google Colab  

  - CaliforniaHousing example: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202207DemoNotebook_CaliforniaHousing.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  
  - BikeSharing example: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202207DemoNotebook_BikeSharing.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  
  - TaiwanCredit example: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202207DemoNotebook_TaiwanCredit.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  
  
</details>  


<br /> 

<details open>
  <summary><h3><strong>Machine Learning Model Validation for Critical or Regulated Applications, June 29 - July 6, 2022 | Online</strong></h3></summary><br /> 

**URL:** [QU-ML Model Validation Workshop](https://mlmodelvalidation.splashthat.com/)

**Speakers:** Agus Sudjianto and Aijun Zhang

**Slides:** 
  - [Part 1: Machine Learning Interpretability](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202206Workshop-Part1%20ML%20Interpretability.pdf) 
  - [Part 2: Model Diagnostics and Validation](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202206Workshop-Part2%20Model%20Diagnostics.pdf)

**Codes:** click the ipynb links to run examples in Google Colab
  - Part 1 - BikeSharing example: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202206Workshop-Part1%20InterpretableML_BikeSharing.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a> 
  - Part 1 - TaiwanCredit example: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202206Workshop-Part1%20InterpretableML_TaiwanCredit.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  
  - Part 2 - CaliforniaHousing example: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202206Workshop-Part2%20Model_Diagnostics_CaliforniaHousing.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  
  - Part 2 - BikeSharing example: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202206Workshop-Part2%20Model_Diagnostics_BikeSharing.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  
  - Part 2 - TaiwanCredit example: <a style="text-laign: 'center'" target="_blank" href="https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202206Workshop-Part2%20Model_Diagnostics_TaiwanCredit.ipynb"><img src="https://github.com/SelfExplainML/PiML-Toolbox/blob/main/examples/results/LogoColab.png" width="20">  ipynb</a>  

Machine Learning (ML) has gained significant adoption in the industry; however, many concerns remained for their usage in highly regulated or critical applications. As with any models, understanding and testing the risk of ML are front and center in the discipline of model risk management. Given the data driven approach as well as the complexity of ML algorithms, we need to improve the sophistication of model design and validation to evaluate both their conceptual soundness as well as outcome. Key element for the conceptual soundness evaluation is model explainability/interpretability. Comprehensive Machine Learning model validation for real production require analysis beyond the standard model performance evaluation which must cover: identification of model weakness through residual slicing, identification of overfitting and underfitting regions, model robustness under noisy or corrupted inputs, prediction reliability such as evaluation of prediction uncertainty and resilience of model performance under input distribution drift. PiML (Python Interpretable Machine Learning) was created to address all the aforementioned needs in a single easy to use (low code) packages.

We are going to cover the above aspects — including hands on experience using PiML — in a two-session seminar:

**Session 1: Machine Learning Interpretability**

- Post-hoc explainability tools for black box models
  - Local explainability: LIME and SHAP
  - Global explainability: Variable Importance (VI), PDP and ALE
- Limitation and pitfalls of post hoc explainability
- Deep ReLU Networks as Inherehently Interpretable Models
  - Local Linear Model Representation of ReLU DNN and Interpretability
  - Controlling Model Complexity through Regularization
- Functional ANOVA (FANOVA) and Interpretable Model Representation
  - Explainable Boosting Machine
  - Generalized Additive Model with structed Interactions (GAMI) Networks

**Session 2: Machine Learning Model Diagnostics and Validation**

- Model Diagnostics and Testing
  - Weak Spot Analysis through Error Slicing
  - Identification of Over and Underfitting Regions
  - Robustness Testing
  - Reliability (Prediction Uncertainty) Testing using Conformal Prediction
  - Resiliency Testing under Input Distribution Drift
- Model Comparison
  - Arbitrary black box vs. Inherently interpretable models
  - Performance, Robustness and Resilience

</details>  


<br /> 
<details open>
  <summary><h3><strong> Machine Learning Model Validation MasterClass, May 9, 2022 | New York City</strong></h3></summary><br />

**URL:** [RISK AMERICAS 2022 Model Validation Workshop](https://www.cefpro.com/forthcoming-events/risk-americas/#section-1643126595388-11-2)

**Speakers:** Agus Sudjianto and Vijay Nair

**Slides:** 
  - [Part 1: Introduction](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part1%20Intro.pdf)
  - [Part 2: Machine Learning Algorithms and Explainability](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part2%20Ml-Algorithms-Explainability.pdf)
  - [Part 3: Unwrapping ReLU Deep Neural Networks](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part3%20ReLU-DNN.pdf)
  - [Part 4: Inherently Interpretable Models, EBM and GAMI-Net](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part4%20EBM%20and%20GAMI-Net.pdf)
  - [Part 5: Outcome Testing](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part5%20Outcome%20Testing.pdf)

The focus of this workshop is to provide comprehensive approach for machine learning model validation with special emphasis on conceptual soundness and outcome analysis. The key elements includes: model explainability, model weakness identification, prediction reliability, model robustness under changing environment and fairness. The workshop will spend significant amount of time on inherently interpretable models due to their key role for high risk applications as well as model benchmarks. This is a hands on workshop where the participants will learn practical concepts along with exercise using Python in Google Colab. Low code python packages will be provided so that participants with minimum familiarity of Python will be able to follow without difficulty.
  
**Session 1: Introduction and machine learning explainability**

- Elements of machine learning validation: Conceptual soundness and outcome analysis
- Introduction to key concepts: explainability, robustness, reliability and fairness
- Post-hoc explainability tools
  - Local explainability: LIME and SHAP 
  - Global explainability: Variable Importance,  Partial Dependence and Accumulated Local Effects

**Session 2: Designing inherently interpretable model**

- Limitation of post-hoc explainability
- Introduction to building inherently interpretable model
  - Explainable boosting machine
  - GAMI Neural Networks

**Session 3: Deep ReLU networks as interpretable models**

- Local partition and linear models
- Model interpretation and diagnostics
- Complexity control through regularization

**Session 4: Outcome testing**

- Identification of performance weakness through slicing
- Reliability evaluation through conformal prediction
- Robustness evaluation for covariate/distribution drift
- Fairness testing

</details>  
