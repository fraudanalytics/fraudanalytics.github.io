<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>PiML Toolbox</title>
  

  
  <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/plot_directive.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/thebelab.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
<script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
<script src="../../_static/jquery.js"></script>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script src="../../_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>

</head>

<body class="wy-body-for-nav">

  


<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../../index.html">
        <img
          class="sk-brand-img"
          src="../../_static/piml-logo.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../install.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../user_guide.html">User Guide</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../modules/classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../auto_examples/index.html">Examples</a>
        </li>
     </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Go" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
  <div class="d-flex" id="sk-doc-wrapper">
      <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
      <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Toggle Menu</label>
      <div id="sk-sidebar-wrapper" class="border-right">
        <div class="sk-sidebar-toc-wrapper">
          <div class="sk-sidebar-toc-logo">
            <a href="../../index.html">
              <img
                class="sk-brand-img"
                src="../../_static/piml-logo.png"
                alt="logo"/>
            </a>
          </div>
          <!--div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
              <a href="data_quality_drift.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="2.7. Data Quality (Drift Test)">Prev</a><a href="../data.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="2. Data Pipeline">Up</a>
              <a href="../train.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="3. Model Train and Tune">Next</a>
          </div-->
              <div class="sk-sidebar-toc">
              
                <ul>
                
                
                
                
                
                
                <li>
                  <a href="../../user_guide.html" class="sk-toc-active">User Guide</a>
                </li>
                <ul>
                
                  <li>
                    <a href="../introduction.html" class="">1. Introduction</a>
                    
                  </li>
                
                  <li>
                    <a href="../data.html" class="sk-toc-active">2. Data Pipeline</a>
                    
                    <ul>
                      
                        <li class="sk-toctree-l3">
                          <a href="data_load.html">2.1. Data Load</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="data_summary.html">2.2. Data Summary</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="data_prepare.html">2.3. Data Preparation</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="data_eda.html">2.4. Exploratory Analysis</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="data_quality_integrity.html">2.5. Data Quality (Integrity Check)</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="data_quality_outlier.html">2.6. Data Quality (Outlier Detection)</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="data_quality_drift.html">2.7. Data Quality (Drift Test)</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="">2.8. Feature Selection</a>
                        </li>
                      
                    </ul>
                    
                  </li>
                
                  <li>
                    <a href="../train.html" class="">3. Model Train and Tune</a>
                    
                  </li>
                
                  <li>
                    <a href="../explain.html" class="">4. Post-hoc Explainability</a>
                    
                  </li>
                
                  <li>
                    <a href="../models.html" class="">5. Interpretable Models</a>
                    
                  </li>
                
                  <li>
                    <a href="../testing.html" class="">6. Diagnostic Suite</a>
                    
                  </li>
                
                  <li>
                    <a href="../compare.html" class="">7. Model Comparison</a>
                    
                  </li>
                
                  <li>
                    <a href="../cases.html" class="">8. Case Studies</a>
                    
                  </li>
                
                </ul>
                
                
                
                
                </ul>
              </div>
        </div>
      </div>
      <div id="sk-page-content-wrapper">
        <div class="sk-page-content container-fluid body px-md-3" role="main">
          
  <style type="text/css">
  div.body div.toctree-wrapper ul {
      padding-left: 0;
  }

  div.body li.toctree-l1 {
      padding: 0 0 0.5em 0;
      list-style-type: none;
      font-size: 150%;
      font-weight: bold;
  }

  div.body li.toctree-l2 {
      font-size: 70%;
      list-style-type: square;
      font-weight: normal;
      margin-left: 40px;
  }

  div.body li.toctree-l3 {
      font-size: 85%;
      list-style-type: circle;
      font-weight: normal;
      margin-left: 40px;
  }

  div.body li.toctree-l4 {
      margin-left: 40px;
  }

</style><section id="feature-selection">
<h1><span class="section-number">2.8. </span>Feature Selection<a class="headerlink" href="#feature-selection" title="Permalink to this heading">¶</a></h1>
<p>Feature selection aims at selecting a subset of covariates that are most relevant to the response. When the number of features is large, feature selection can help mitigate computational burden and avoid overfitting. Moreover, reducing the number of modeling features is also beneficial for enhancing model interpretability.</p>
<p>PIML has four built-in feature selection strategies. For demonstration purposes, we run the following example codes to initialize a PiML experiment for the BikeSharing dataset. Note that feature selection is based on training data, and hence the <code class="docutils literal notranslate"><span class="pre">exp.data_prepare</span></code> function should be executed before running <code class="docutils literal notranslate"><span class="pre">exp.feature_select</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In all four strategies, we do not distinguish numerical features and categorical features. This treatment is not rigorous, especially for Pearson correlation, distance correlation, and rcit tests.</p>
</div>
<section id="correlations">
<h2><span class="section-number">2.8.1. </span>Correlations<a class="headerlink" href="#correlations" title="Permalink to this heading">¶</a></h2>
<p><strong>Pearson correlation</strong> is a measure of the linear relationship between two continuous variables. Mathematically, the correlation coefficient of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> can be calculated in the following way.</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\rho_{X Y}=\frac{\sum_{i=1}^n\left(X_i-\bar{X}\right)\left(Y_i-\bar{Y}\right)}
         {\sqrt{\sum_{i=1}^n\left(X_i-\bar{X}\right)^2} \sqrt{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}}
\end{align}\]</div>
<p>The value of <span class="math notranslate nohighlight">\(\rho_{X Y}\)</span> ranges from -1 to 1, where the sign denotes the direction of the relationship, and the magnitude represents the correlation strength. The corresponding feature selection strategy is straightforward:</p>
<blockquote>
<div><ul class="simple">
<li><p>Calculate the Pearson correlation coefficient between each covariate and the response.</p></li>
<li><p>Select features with <span class="math notranslate nohighlight">\(|\rho_{X Y}|\)</span> greater than a user-specified <code class="docutils literal notranslate"><span class="pre">threshold</span></code>.</p></li>
</ul>
</div></blockquote>
<p>These two steps are wrapped up and can be called in a single-line command in PiML, as follows,</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;cor&quot;</span><span class="p">,</span> <span class="n">corr_algorithm</span><span class="o">=</span><span class="s2">&quot;pearson&quot;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/0_data/plot_5_feature_select.html"><img alt="../../_images/sphx_glr_plot_5_feature_select_001.png" src="../../_images/sphx_glr_plot_5_feature_select_001.png" /></a>
</figure>
<p>where “cor” is the keyword of the correlation strategy, “pearson” specifies the Pearson correlation, and 0.1 is the user-defined threshold. The output results include:</p>
<blockquote>
<div><ul class="simple">
<li><p>The upper-left figure shows the top 10 most important features, where the blue and orange bars denote the positive and negative correlations, respectively.</p></li>
<li><p>The upper-right table contains the correlation coefficients of all features.</p></li>
<li><p>The bottom line text highlights the selected features.</p></li>
</ul>
</div></blockquote>
<p>Pearson correlation is easy to compute and interpret. However, it cannot measure non-linear relationships, so its usage is limited. One can use the following three methods when dealing with more complex data relationships.</p>
<p><strong>Spearman correlation</strong> is defined as the Pearson correlation between the ranks of the variables, which can be calculated as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{align}
r_{s}=\rho_{\mathrm{R}(X), \mathrm{R}(Y)}= 1-\frac{6 \sum d_i^2}{n\left(n^2-1\right)}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{R}(X), \mathrm{R}(Y)\)</span> are the ranking variables of <span class="math notranslate nohighlight">\(X, Y\)</span>, respectively. <span class="math notranslate nohighlight">\(d_i=\mathrm{R}\left(X_i\right)-\mathrm{R}\left(Y_i\right)\)</span> denotes the difference between the two ranks of the <span class="math notranslate nohighlight">\(i\)</span>-th observation. Spearman correlation measures the strength and direction of the relationship between two variables by evaluating how well the relationship between them can be described by a monotonic function. A perfect Spearman correlation of +1 or -1 occurs when each variable is a perfect monotonic function of the other, assuming there are no repeated data values. The corresponding feature selection strategy is similar to the Pearson correlation, except that we replace the keyword “pearson” with “spearman” in the <code class="docutils literal notranslate"><span class="pre">corr_algorithm</span></code> argument.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;cor&quot;</span><span class="p">,</span> <span class="n">corr_algorithm</span><span class="o">=</span><span class="s2">&quot;spearman&quot;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/0_data/plot_5_feature_select.html"><img alt="../../_images/sphx_glr_plot_5_feature_select_002.png" src="../../_images/sphx_glr_plot_5_feature_select_002.png" /></a>
</figure>
</section>
<section id="distance-correlation">
<h2><span class="section-number">2.8.2. </span>Distance Correlation<a class="headerlink" href="#distance-correlation" title="Permalink to this heading">¶</a></h2>
<p>This is a measure of dependence between two paired random vectors. It can handle both linear and non-linear relationships. Given two features <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, we first calculate their pairwise Euclidean distance matrix, as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    &amp; a_{j, k}=\left\|X_j-X_k\right\|, \quad j, k=1,2, \ldots, n \\
    &amp; b_{j, k}=\left\|Y_j-Y_k\right\|, \quad j, k=1,2, \ldots, n.
\end{aligned}\end{split}\]</div>
<p>Then, these two matrices are centered,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    &amp; A_{j, k}:=a_{j, k}-\bar{a}_{j \cdot}-\bar{a}_{\cdot k}+\bar{a}_{\cdot \cdot} \\
    &amp; B_{j, k}:=b_{j, k}-\bar{b}_{j \cdot}-\bar{b}_{\cdot k}+\bar{b}_{\cdot \cdot}.
\end{aligned}\end{split}\]</div>
<p>The squared sample distance covariance is then defined as the arithmetic average of the products of <span class="math notranslate nohighlight">\(A_{j, k}\)</span> and <span class="math notranslate nohighlight">\(A_{j, k}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
     \mathrm{dCov}^2(X, Y):=\frac{1}{n^2} \sum_{j=1}^n \sum_{k=1}^n A_{j, k} B_{j, k}.
\end{aligned}\]</div>
<p>Finally, the distance correlation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> can be calculated via the following formula.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
     \mathrm{dCor}^2(X, Y) = \frac{\mathrm{dCov}^2(X, Y)}{\sqrt{\mathrm{dVar}^2(X) \operatorname{dVar}^2(Y)}}.
 \end{aligned}\]</div>
<p>Similar to that of Pearson correlation, we first calculate the distance correlation of each covariate and the response and then use a <code class="docutils literal notranslate"><span class="pre">threshold</span></code> to determine which features are selected. Note that the distance correlation is always positive, ranging from 0 to 1, and hence we do not need to take the absolute value of these coefficients. In PiML, this method can be called using the following command,</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;dcor&quot;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/0_data/plot_5_feature_select.html"><img alt="../../_images/sphx_glr_plot_5_feature_select_003.png" src="../../_images/sphx_glr_plot_5_feature_select_003.png" /></a>
</figure>
<p>where the keyword is “dcor”.</p>
<p>The main advantage of distance correlation is that it can capture non-linear relationships. However, its calculation requires the pairwise distance matrix, which can be computationally very expensive when the sample size is large. To make the computation scalable for big data, we downsample 5000 samples from the original data and use them to calculate the distance correlation. The distance correlation is calculated using the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> package by default. Alternatively, if the <code class="docutils literal notranslate"><span class="pre">dcor</span></code> package is installed, then the <code class="docutils literal notranslate"><span class="pre">dcor</span></code> package will be used automatically due to speed considerations.</p>
</section>
<section id="use-of-feature-importance">
<h2><span class="section-number">2.8.3. </span>Use of Feature Importance<a class="headerlink" href="#use-of-feature-importance" title="Permalink to this heading">¶</a></h2>
<p>This strategy is based on first fitting an ML model and using a post hoc method for selecting the important variables. It is composed of the following four steps:</p>
<blockquote>
<div><ul class="simple">
<li><p>Fit an XGB model using all the covariates and the response.</p></li>
<li><p>Run permutation feature importance test for the fitted XGB model, and get the importance of each feature.</p></li>
<li><p>Sort features in the descending order of importance (normalized such that all values sum to 1).</p></li>
<li><p>Select the top features with accumulated importance greater than a pre-defined threshold.</p></li>
</ul>
</div></blockquote>
<p>See the example usage below,</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;pfi&quot;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/0_data/plot_5_feature_select.html"><img alt="../../_images/sphx_glr_plot_5_feature_select_004.png" src="../../_images/sphx_glr_plot_5_feature_select_004.png" /></a>
</figure>
<p>where the keyword here is “pfi”, and this test is based on the implementation of scikit-learn, and the details can be found at <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html">https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html</a>. The setting <code class="docutils literal notranslate"><span class="pre">threshold=0.95</span></code> means selecting the top features with the sum of feature importance greater than 95%. The valid range of <code class="docutils literal notranslate"><span class="pre">threshold</span></code> is 0 to 1, representing the percentage of accumulated importance. As we increase the threshold, more features would be selected.</p>
<p>By using XGB and PFI to rank features, the selected features are most relevant for predicting the response. There are several concerns with this approach: i) it is a post hoc method in that one has to fit a model first and use it to select the important variables; ii) the results can vary with the particular algorithm used; and iii) the fitted model can overfit or underfit, so the identified variables may not be the right ones.</p>
</section>
<section id="randomized-conditional-independence-test">
<h2><span class="section-number">2.8.4. </span>Randomized Conditional Independence Test<a class="headerlink" href="#randomized-conditional-independence-test" title="Permalink to this heading">¶</a></h2>
<p>This strategy aims to identify a Markov boundary of the response variable, where the Markov boundary is defined as the minimal feature subset with maximum predictive power. In particular, the randomized conditional independence test (RCIT) is used to test whether a variable is probabilistically independent of the response variable, conditioning on the Markov boundary. A forward-backward selection strategy is incorporated with RCIT to generate the Markov boundary.</p>
<section id="rcit-test">
<h3><span class="section-number">2.8.4.1. </span>RCIT Test<a class="headerlink" href="#rcit-test" title="Permalink to this heading">¶</a></h3>
<p>Given a Markov boundary set <span class="math notranslate nohighlight">\(Z\)</span>, the goal is to test whether a feature <span class="math notranslate nohighlight">\(X\)</span> is independent of the response variable <span class="math notranslate nohighlight">\(Y\)</span>, namely <span class="math notranslate nohighlight">\(X \perp Y \mid Z\)</span>. The RCIT test is highly related to KCIT, see follows.</p>
<blockquote>
<div><ul class="simple">
<li><p>KCIT <a class="reference internal" href="#zhang2012" id="id1"><span>[Zhang2012]</span></a>: kernel conditional independent test, works for non-linear and arbitrary data distribution; but not scalable.</p></li>
<li><p>RCIT <a class="reference internal" href="#strobl2019" id="id2"><span>[Strobl2019]</span></a>: a fast approximation of KCIT; random Fourier features are used instead of reproducing kernel Hibert spaces.</p></li>
</ul>
</div></blockquote>
<p>Therefore, RCIT is used as it can handle non-linear relationships and is fast in computation (as compared to KCIT).
A detailed introduction to RCIT is given below:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X, Y, Z\)</span> are first transformed using random Fourier features;</p></li>
<li><p>Then the null hypothesis <span class="math notranslate nohighlight">\(X \perp Y \mid Z\)</span> is equivalent to zero partial cross-covariance <span class="math notranslate nohighlight">\(\Sigma_{X Y \mid Z}=\Sigma_{X Y}-\Sigma_{Y Z} \Sigma_{Z Z}^{-1} \Sigma_{X Z}=0\)</span>;</p></li>
<li><p>The test statistics is then approximated by <span class="math notranslate nohighlight">\(\left\|\hat{\Sigma}_{X Y \mid Z}\right\|_F^2=\mathrm{n} \hat{\Sigma}_{X Y}-\hat{\Sigma}_{Y Z}\left(\hat{\Sigma}_{Z Z}+\gamma I\right)^{-1} \hat{\Sigma}_{X Z}\)</span>;</p></li>
<li><p>The asymptotic distribution of the test statistics is <span class="math notranslate nohighlight">\(\sum_{i=1} \lambda_i z_i^2\)</span>, where <span class="math notranslate nohighlight">\(z_i\)</span> are i.i.d. standard Gaussian variables.</p></li>
<li><p>Lindsay-Pilla-Basak (LPB; <a class="reference internal" href="#lindsayl2000" id="id3"><span>[Lindsayl2000]</span></a>) approximates the CDF under the null using a finite mixture of Gamma distribution.</p></li>
</ul>
</div></blockquote>
<p>In PiML, the number of random Fourier features is set to 100 (<span class="math notranslate nohighlight">\(Z\)</span>) and 5 (<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>), respectively.</p>
</section>
<section id="forward-backward-selection-with-early-dropping-fbedk">
<h3><span class="section-number">2.8.4.2. </span>Forward-Backward selection with Early Dropping (FBEDk):<a class="headerlink" href="#forward-backward-selection-with-early-dropping-fbedk" title="Permalink to this heading">¶</a></h3>
<p>The FBEDk <a class="reference internal" href="#borboudakis2019" id="id4"><span>[Borboudakis2019]</span></a> algorithm is a combination of forward selection and backward elimination. Here we first run forward selection with a user-defined Markov boundary set as initial and then conduct backward elimination to further delete insignificant features.</p>
<p><strong>Forward Selection</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Given a predefined Markov boundary set, we initialize all the remaining covariate features as candidate features.</p></li>
<li><p>Run the RCIT test between each candidate feature and the response variable, conditional on the Markov boundary set.</p></li>
<li><p>Features with <code class="docutils literal notranslate"><span class="pre">p_value</span> <span class="pre">&lt;=</span> <span class="pre">threshold</span></code> will be selected as the candidate features.</p></li>
<li><p>Among the candidate features, the most significant one will be added to the Markov boundary set.</p></li>
<li><p>Repeat the last three steps, and the algorithm stops as the candidate set is empty.</p></li>
</ul>
</div></blockquote>
<p>The above steps describe one run of the forward phase. To increase accuracy, the overall forward phase is repeated for <code class="docutils literal notranslate"><span class="pre">k</span></code> times, i.e., the character “k” in “FBEDk”. As recommended by <a class="reference internal" href="#yu2020" id="id5"><span>[Yu2020]</span></a>, the forward phase is repeated twice, i.e., the value of <code class="docutils literal notranslate"><span class="pre">k</span></code> is 2, and you may change this parameter by specifying the argument <code class="docutils literal notranslate"><span class="pre">n_forward_phase</span></code>.</p>
<p><strong>Backward Elimination</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Temporarily remove feature <span class="math notranslate nohighlight">\(j\)</span> from the Markov boundary set.</p></li>
<li><p>Run RCIT test for feature <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, conditional on the temporary Markov boundary set.</p></li>
<li><p>Permanently remove feature <span class="math notranslate nohighlight">\(j\)</span> from the Markov boundary set, if <code class="docutils literal notranslate"><span class="pre">p_value</span> <span class="pre">&gt;</span> <span class="pre">threshold</span></code>.</p></li>
</ul>
</div></blockquote>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;rcit&quot;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_forward_phase</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/0_data/plot_5_feature_select.html"><img alt="../../_images/sphx_glr_plot_5_feature_select_005.png" src="../../_images/sphx_glr_plot_5_feature_select_005.png" /></a>
</figure>
<p>The keyword here is “rcit”. The upper-left figure shows the step-by-step formulation of the Markov boundary set. The value of the <code class="docutils literal notranslate"><span class="pre">threshold</span></code> should be between 0 to 1. The smaller its value, the fewer features would be marked as significant and then selected.</p>
<p>In the beginning, the Markov boundary set is empty, and the RCIT test is run over each feature and the response.
As shown in iteration zero, some features are marked as significant if the corresponding <code class="docutils literal notranslate"><span class="pre">p_value</span> <span class="pre">&lt;=</span> <span class="pre">threshold</span></code>, e.g., <code class="docutils literal notranslate"><span class="pre">hr</span></code> and <code class="docutils literal notranslate"><span class="pre">temp</span></code>; while <code class="docutils literal notranslate"><span class="pre">workingday</span></code> and <code class="docutils literal notranslate"><span class="pre">weekday</span></code> are shown to be insignificant. In iteration one, the most significant feature <code class="docutils literal notranslate"><span class="pre">temp</span></code> is added to the Markov boundary set, while <code class="docutils literal notranslate"><span class="pre">workingday</span></code> and <code class="docutils literal notranslate"><span class="pre">weekday</span></code> are then removed from the candidate feature list. This procedure will be repeated, and in the end, seven features are selected, including, <code class="docutils literal notranslate"><span class="pre">temp</span></code>, <code class="docutils literal notranslate"><span class="pre">hr</span></code>, <code class="docutils literal notranslate"><span class="pre">yr</span></code>, <code class="docutils literal notranslate"><span class="pre">weathersit</span></code>, <code class="docutils literal notranslate"><span class="pre">season</span></code>, <code class="docutils literal notranslate"><span class="pre">hum</span></code>, and <code class="docutils literal notranslate"><span class="pre">mnth</span></code>.</p>
<p>As the FBEDk algorithm can start with an arbitrary Markov boundary set, users may define a non-empty Markov boundary set as the start point, see the following example.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;rcit&quot;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">preset</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;hr&quot;</span><span class="p">,</span> <span class="s2">&quot;temp&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../auto_examples/0_data/plot_5_feature_select.html"><img alt="../../_images/sphx_glr_plot_5_feature_select_006.png" src="../../_images/sphx_glr_plot_5_feature_select_006.png" /></a>
</figure>
<p>This time, two features <code class="docutils literal notranslate"><span class="pre">hr</span></code> and <code class="docutils literal notranslate"><span class="pre">temp</span></code> are selected as the initialization, and they are shown in deep blue in iteration 0.</p>
<p>The RCIT-based feature selection method is capable of handling non-linear relationships, and the selected features are all causally related to the response, subject to the pre-defined significance level. The disadvantages of this method include: a) the computational burden is relatively high; b) as it is a sequential selection approach, the results may be slightly different as we use different initial Markov boundary sets.</p>
</section>
</section>
<section id="examples">
<h2><span class="section-number">2.8.5. </span>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h2>
<p>The full example codes of this section can be found in the following link.</p>
<aside class="topic">
<p class="topic-title">Example</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../auto_examples/0_data/plot_5_feature_select.html#sphx-glr-auto-examples-0-data-plot-5-feature-select-py"><span class="std std-ref">Feature Selection</span></a></p></li>
</ul>
</aside>
<aside class="topic">
<p class="topic-title">References</p>
<div role="list" class="citation-list">
<div class="citation" id="zhang2012" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">Zhang2012</a><span class="fn-bracket">]</span></span>
<p>Kun Zhang, Jonas Peters, Dominik Janzing, Bernhard Schoelkopf (2012). <a class="reference external" href="https://arxiv.org/ftp/arxiv/papers/1202/1202.3775.pdf">Kernel-based conditional independence test and application in causal discovery</a>, arXiv preprint arXiv:1202.3775.</p>
</div>
<div class="citation" id="strobl2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">Strobl2019</a><span class="fn-bracket">]</span></span>
<p>Eric V. Strobl, Kun Zhang, Shyam Visweswaran (2019). <a class="reference external" href="https://arxiv.org/pdf/1702.03877.pdf">Approximate kernel-based conditional independence tests for fast non-parametric causal discovery</a>, Journal of Causal Inference, 7(1).</p>
</div>
<div class="citation" id="lindsayl2000" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Lindsayl2000</a><span class="fn-bracket">]</span></span>
<p>Bruce G. Lindsay, Ramani S. Pilla &amp; Prasanta Basak (2000). <a class="reference external" href="https://link.springer.com/article/10.1023/A:1004105603806">Moment-based approximations of distributions using mixtures: Theory and applications</a>, Annals of the Institute of Statistical Mathematics, 52(2), pp.215-230.</p>
</div>
<div class="citation" id="borboudakis2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Borboudakis2019</a><span class="fn-bracket">]</span></span>
<p>Giorgos Borboudakis, Ioannis Tsamardinos (2019). <a class="reference external" href="https://www.jmlr.org/papers/volume20/17-334/17-334.pdf">Forward-backward selection with early dropping</a>, The Journal of Machine Learning Research 20(1), pp.276-314.</p>
</div>
<div class="citation" id="yu2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">Yu2020</a><span class="fn-bracket">]</span></span>
<p>Kui Yu, Xianjie Guo, Lin Liu, Jiuyong Li, Hao Wang, Zhaolong Ling, Xindong Wu (2020). <a class="reference external" href="https://arxiv.org/pdf/1911.07147.pdf">Causality-based feature selection: Methods and evaluations</a>. ACM Computing Surveys (CSUR), 53(5), pp.1-36.</p>
</div>
</div>
</aside>
</section>
</section>


        </div>
      <div class="container">
        <footer class="sk-content-footer">
              &copy; Copyright 2022-, PiML-Toolbox authors.
            <a href="../../_sources/guides/data/feature_select.rst.txt" rel="nofollow">Show this page source</a>
        </footer>
      </div>
    </div>
  </div>


  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
      
      const tooltipTriggerList = document.querySelectorAll('[data-bs-toggle="tooltip"]')
      const tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl))
  </script>

<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    

<script src="_static/js/vendor/bootstrap.min.js"></script>

</body>
</html>