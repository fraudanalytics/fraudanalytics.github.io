<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>PiML Toolbox</title>
  

  
  <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/plot_directive.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/thebelab.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
<script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
<script src="../../_static/jquery.js"></script>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script src="../../_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>

</head>

<body class="wy-body-for-nav">

  


<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../../index.html">
        <img
          class="sk-brand-img"
          src="../../_static/piml-logo.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../install.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../user_guide.html">User Guide</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../modules/classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../auto_examples/index.html">Examples</a>
        </li>
     </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Go" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
  <div class="d-flex" id="sk-doc-wrapper">
      <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
      <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Toggle Menu</label>
      <div id="sk-sidebar-wrapper" class="border-right">
        <div class="sk-sidebar-toc-wrapper">
          <div class="sk-sidebar-toc-logo">
            <a href="../../index.html">
              <img
                class="sk-brand-img"
                src="../../_static/piml-logo.png"
                alt="logo"/>
            </a>
          </div>
          <!--div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
              <a href="../models.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="5. Interpretable Models">Prev</a><a href="../models.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="5. Interpretable Models">Up</a>
              <a href="gam.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="5.2. Generalized Additive Model">Next</a>
          </div-->
              <div class="sk-sidebar-toc">
              
                <ul>
                
                
                
                
                
                
                <li>
                  <a href="../../user_guide.html" class="sk-toc-active">User Guide</a>
                </li>
                <ul>
                
                  <li>
                    <a href="../introduction.html" class="">1. Introduction</a>
                    
                  </li>
                
                  <li>
                    <a href="../data.html" class="">2. Data Pipeline</a>
                    
                  </li>
                
                  <li>
                    <a href="../train.html" class="">3. Model Train and Tune</a>
                    
                  </li>
                
                  <li>
                    <a href="../explain.html" class="">4. Post-hoc Explainability</a>
                    
                  </li>
                
                  <li>
                    <a href="../models.html" class="sk-toc-active">5. Interpretable Models</a>
                    
                    <ul>
                      
                        <li class="sk-toctree-l3">
                          <a href="">5.1. Generalized Linear Models</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="gam.html">5.2. Generalized Additive Model</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="tree.html">5.3. Decision Tree</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="figs.html">5.4. Fast Interpretable Greedy-tree Sums</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="xgb1.html">5.5. XGBoost Depth 1</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="xgb2.html">5.6. XGBoost Depth 2</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="ebm.html">5.7. Explainable Boosting Machines</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="gaminet.html">5.8. GAMI-Net</a>
                        </li>
                      
                        <li class="sk-toctree-l3">
                          <a href="reludnn.html">5.9. ReLU Neural Network</a>
                        </li>
                      
                    </ul>
                    
                  </li>
                
                  <li>
                    <a href="../testing.html" class="">6. Diagnostic Suite</a>
                    
                  </li>
                
                  <li>
                    <a href="../compare.html" class="">7. Model Comparison</a>
                    
                  </li>
                
                  <li>
                    <a href="../cases.html" class="">8. Case Studies</a>
                    
                  </li>
                
                </ul>
                
                
                
                
                </ul>
              </div>
        </div>
      </div>
      <div id="sk-page-content-wrapper">
        <div class="sk-page-content container-fluid body px-md-3" role="main">
          
  <style type="text/css">
  div.body div.toctree-wrapper ul {
      padding-left: 0;
  }

  div.body li.toctree-l1 {
      padding: 0 0 0.5em 0;
      list-style-type: none;
      font-size: 150%;
      font-weight: bold;
  }

  div.body li.toctree-l2 {
      font-size: 70%;
      list-style-type: square;
      font-weight: normal;
      margin-left: 40px;
  }

  div.body li.toctree-l3 {
      font-size: 85%;
      list-style-type: circle;
      font-weight: normal;
      margin-left: 40px;
  }

  div.body li.toctree-l4 {
      margin-left: 40px;
  }

</style><section id="generalized-linear-models">
<h1><span class="section-number">5.1. </span>Generalized Linear Models<a class="headerlink" href="#generalized-linear-models" title="Permalink to this heading">¶</a></h1>
<p>Generalized linear models (GLMs) are well known in the statistical literature and are among the most interpretable models. Readers should consult <a class="reference internal" href="#mccullagh1989" id="id1"><span>[McCullagh1989]</span></a> for details.</p>
<p>GLMs are based on the assumption that, after a suitable transformation of the expected response, it has a linear relationship with the <span class="math notranslate nohighlight">\(d\)</span>-dimensional features <span class="math notranslate nohighlight">\(x\)</span>. More specifically, the relationship is of the following form,</p>
<div class="math notranslate nohighlight">
\[\begin{align}
   g(\mathbb{E}(y|\textbf{x})) =  \mu +  w_1 x_1 + w_2 x_2 + \ldots + w_d x_d,  \tag{1}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the intercept term, <span class="math notranslate nohighlight">\(w_1, w_2, \ldots, w_d\)</span> are model coefficients (also called weights), and <span class="math notranslate nohighlight">\(g(\cdot)\)</span> is the link function. GLMs can be used to analyze continuous responses as well as binary, categorical, and count responses. The most popular link functions are the identity function <span class="math notranslate nohighlight">\(g(x)=x\)</span> for continuous responses, and the logit function <span class="math notranslate nohighlight">\(g(x)=\log(x/(1-x))\)</span> for binary responses.</p>
<section id="model-training">
<h2><span class="section-number">5.1.1. </span>Model Training<a class="headerlink" href="#model-training" title="Permalink to this heading">¶</a></h2>
<p>There are many existing packages for fitting GLMs under various settings and options. PiML integrates the scikit-learn implementations into <code class="docutils literal notranslate"><span class="pre">GLMRegressor</span></code> and <code class="docutils literal notranslate"><span class="pre">GLMClassifier</span></code>, with L1 and L2 regularization options.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">l1_regularization</span></code>: The regularization strength of the L1 penalty, which penalizes the absolute values of model coefficients.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">l2_regularization</span></code>: The regularization strength of the L2 penalty, which penalizes the squared values of model coefficients.</p></li>
</ul>
<p>Both of these two regularization shrink the regression coefficients toward zero. With large L1 regularization, the fitted coefficients would become sparse. In contrast, with large L2 regularization, the fitted coefficients will be shrunk towards zero.
The different combinations of these two hyperparameters would lead to different variants of GLMs.</p>
<p><a class="reference external" href="../../modules/generated/piml.models.GLMRegressor.html">GLMRegressor</a> calls the following modules of scikit-learn.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression">sklearn.linear_model.LinearRegression</a>: Linear regression without constraint (<code class="docutils literal notranslate"><span class="pre">l1_regularization=0</span></code>, and <code class="docutils literal notranslate"><span class="pre">l2_regularization=0</span></code>).</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso">sklearn.linear_model.Lasso</a>: Linear regression with L1 regularizer (<code class="docutils literal notranslate"><span class="pre">l1_regularization&gt;0</span></code>, and <code class="docutils literal notranslate"><span class="pre">l2_regularization=0</span></code>).</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge">sklearn.linear_model.Ridge</a>: Linear regression with L2 regularizer (<code class="docutils literal notranslate"><span class="pre">l1_regularization=0</span></code>, and <code class="docutils literal notranslate"><span class="pre">l2_regularization&gt;0</span></code>).</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet">sklearn.linear_model.ElasticNet</a>: Linear regression with elastic net regularizer (<code class="docutils literal notranslate"><span class="pre">l1_regularization&gt;0</span></code>, and <code class="docutils literal notranslate"><span class="pre">l2_regularization&gt;0</span></code>).</p></li>
</ul>
<p>For binary regression (called classification in the ML literature), <a class="reference external" href="../../modules/generated/piml.models.GLMClassifier.html">GLMClassifier</a> directly uses the LogisticRegression class of scikit-learn.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression">sklearn.linear_model.LogisticRegression</a>: Logistic regression classifier with both L1 and L2 regularizer.</p></li>
</ul>
<p>The following codes can be used to do model training.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">piml.models</span> <span class="kn">import</span> <span class="n">GLMRegressor</span>
<span class="n">exp</span><span class="o">.</span><span class="n">model_train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">GLMRegressor</span><span class="p">(</span><span class="n">l1_regularization</span><span class="o">=</span><span class="mf">0.0008</span><span class="p">,</span> <span class="n">l2_regularization</span><span class="o">=</span><span class="mf">0.0008</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<p>We can test its performance using the <code class="docutils literal notranslate"><span class="pre">model_diagnose</span></code> function.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_diagnose</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;accuracy_table&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<p>Using this command, a table would be generated, showing model performance on both train and test sets.</p>
</section>
<section id="global-interpretation">
<h2><span class="section-number">5.1.2. </span>Global Interpretation<a class="headerlink" href="#global-interpretation" title="Permalink to this heading">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">exp.model_interpret</span></code> function provides the capability to interpret a fitted model.</p>
<section id="regression-coefficients">
<h3><span class="section-number">5.1.2.1. </span>Regression Coefficients<a class="headerlink" href="#regression-coefficients" title="Permalink to this heading">¶</a></h3>
<p>In  linear models, the regression coefficients estimate the change in the response variable for a unit change in the corresponding predictor variable, holding all other variables constant. This interpretation has to be adapted suitably for other link functions used in the GLM. For example,</p>
<ul class="simple">
<li><p>In a logistic regression model with a logit link function, the regression coefficient represents the change in the log odds of the response variable for a one-unit increase in the predictor variable.</p></li>
</ul>
<p>When the predictor variables are measured on different scales, the coefficients cannot be directly compared. To address this issue, we standardize the predictor variables so that we can compare the coefficients on a comparable scale.</p>
<p>In PiML, we use the following command to show the regression coefficients of a fitted GLM, with the keyword “glm_coef_plot”.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_interpret</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;glm_coef_plot&quot;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/3_models/plot_0_glm_reg.html"><img alt="../../_images/sphx_glr_plot_0_glm_reg_001.png" src="../../_images/sphx_glr_plot_0_glm_reg_001.png" /></a>
</figure>
<p>This plot shows the coefficients of the top 10 numerical features in a linear regression model, ranked by their absolute magnitude. A positive coefficient indicates that an increase in the corresponding feature value is associated with an increase in the response variable, while a negative coefficient indicates the opposite.</p>
<p>In this case, the plot suggests that the most important numerical feature in the model is <code class="docutils literal notranslate"><span class="pre">temp</span></code> (temperature), with a positive effect on the response variable. This is followed by <code class="docutils literal notranslate"><span class="pre">hum</span></code> (humidity), which has a negative effect on the response variable, and <code class="docutils literal notranslate"><span class="pre">hr</span></code> (hour), which has a positive effect on the response variable.</p>
<p>It is important to note that, in GLM, categorical variables are preprocessed by one-hot encoding, which involves converting each categorical variable into a set of binary variables representing each possible category. To visualize the coefficients of the categorical variables, a bar chart coefficient plot can be used. This involves plotting the coefficients for each category of the categorical variable separately, using the <code class="docutils literal notranslate"><span class="pre">uni_feature</span></code> parameter to specify the name of the categorical variable.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_interpret</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;glm_coef_plot&quot;</span><span class="p">,</span> <span class="n">uni_feature</span><span class="o">=</span><span class="s2">&quot;season&quot;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/3_models/plot_0_glm.html"><img alt="../../_images/sphx_glr_plot_0_glm_reg_002.png" src="../../_images/sphx_glr_plot_0_glm_reg_002.png" /></a>
</figure>
<p>In this example, the categorical variable <code class="docutils literal notranslate"><span class="pre">season</span></code> has four categories, which are represented by four binary dummy variables: <code class="docutils literal notranslate"><span class="pre">season_1</span></code>, <code class="docutils literal notranslate"><span class="pre">season_2</span></code>, <code class="docutils literal notranslate"><span class="pre">season_3</span></code>, and <code class="docutils literal notranslate"><span class="pre">season_4</span></code>. To avoid overparameterization, only three of the four dummy variables are used for model fitting, and the fourth one (<code class="docutils literal notranslate"><span class="pre">season_1</span></code>) is dropped and assigned a coefficient of zero.
Note that the choice of which dummy variable to drop can affect the interpretation of the coefficients for the remaining variables, especially if there are interactions or nonlinear effects between categories.</p>
<p>To get the results of all features, you can use the keyword “glm_coef_table”, and a data frame containing all features’ importance will be displayed. Instead of printing the table to the screen, you can also export the results to a data object by setting <code class="docutils literal notranslate"><span class="pre">return_data</span></code> to True.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_interpret</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;glm_coef_table&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
</section>
<section id="feature-importance">
<h3><span class="section-number">5.1.2.2. </span>Feature Importance<a class="headerlink" href="#feature-importance" title="Permalink to this heading">¶</a></h3>
<p>The feature importance measures the relative importance of each input feature in predicting the response. Here, the importance of each feature is calculated by measuring the variance of the marginal effect <span class="math notranslate nohighlight">\(w_j x_j, (j = 1,\ldots,d)\)</span> on the training set. For each categorical feature, we aggregate the marginal effects of all of its dummy variables and then calculate the variance. Therefore, the feature importance provides a measure of how much the feature contributes to the overall variability in the model’s predictions. In PiML, we use the keyword “global_fi” to trigger the feature importance plot.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_interpret</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;global_fi&quot;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/3_models/plot_0_glm_reg.html"><img alt="../../_images/sphx_glr_plot_0_glm_reg_003.png" src="../../_images/sphx_glr_plot_0_glm_reg_003.png" /></a>
</figure>
<p>In order to interpret the relative importance of each feature as a proportion of the total importance across all features, we normalize the feature importance so that their sum equals 1. Because there may be many input features in a machine learning model, it can be helpful to focus on the most important features for interpretation and visualization purposes. Therefore, the feature importance plot typically shows only the top 10 most important features, to help identify which features are most relevant for understanding the model’s behavior and making predictions. To get the full results, you can set the parameter <code class="docutils literal notranslate"><span class="pre">return_data</span></code> to True.</p>
</section>
</section>
<section id="local-interpretation">
<h2><span class="section-number">5.1.3. </span>Local Interpretation<a class="headerlink" href="#local-interpretation" title="Permalink to this heading">¶</a></h2>
<p>Local interpretation refers to the local contribution of each feature for a particular prediction on a single sample. In PiML, we can generate the local interpretation plot by keyword “local_fi”, together with the sample id to be interpreted, as follows.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_interpret</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;local_fi&quot;</span><span class="p">,</span> <span class="n">sample_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centered</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">original_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/3_models/plot_0_glm_reg.html"><img alt="../../_images/sphx_glr_plot_0_glm_reg_004.png" src="../../_images/sphx_glr_plot_0_glm_reg_004.png" /></a>
</figure>
<p>In this plot, the bars show the marginal contribution (effects) of each feature. The longer the bar, the larger the marginal effect, and the more contribution the corresponding feature has on the prediction. In contrast, the regression coefficients (weights) indicate the strength and direction of the relationship between each feature and the target variable. The larger the coefficient, the more sensitive the corresponding feature’ contribution to the prediction.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">sample_id=0</span></code> indicates that the plot is showing the marginal effects for the first sample in the training set, and the feature values for this sample are shown on the right axis. Note that this plot only shows the top 10 features with the largest contributions. To get the full results, you can set the parameter <code class="docutils literal notranslate"><span class="pre">return_data</span></code> to True.</p>
<section id="original-scale-option">
<h3><span class="section-number">5.1.3.1. </span>Original Scale Option<a class="headerlink" href="#original-scale-option" title="Permalink to this heading">¶</a></h3>
<p>The right axis of the local interpretation plot shows the scaled feature values. If you want to know the original feature values before the preprocessing, set <code class="docutils literal notranslate"><span class="pre">original_scale=True</span></code>.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_interpret</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;local_fi&quot;</span><span class="p">,</span> <span class="n">sample_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centered</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">original_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/3_models/plot_0_glm_reg.html"><img alt="../../_images/sphx_glr_plot_0_glm_reg_005.png" src="../../_images/sphx_glr_plot_0_glm_reg_005.png" /></a>
</figure>
</section>
<section id="centered-option">
<h3><span class="section-number">5.1.3.2. </span>Centered Option<a class="headerlink" href="#centered-option" title="Permalink to this heading">¶</a></h3>
<p>In GLM, the interpretation of the marginal effects can be subject to model identifiability issues, where the marginal effect of a feature can be directly absorbed into the intercept term without changing the overall model prediction. This can result in unstable or arbitrary interpretations of the marginal effects.</p>
<p>To address this issue, a common approach is to center the features by subtracting the mean value of the feature from each observation. This can help to remove the identifiability problem and provide more stable interpretations of the marginal effects. In PiML, we set <code class="docutils literal notranslate"><span class="pre">centered=True</span></code> to turn on the centering option, as follows.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_interpret</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;local_fi&quot;</span><span class="p">,</span> <span class="n">sample_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centered</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">original_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/3_models/plot_0_glm_reg.html"><img alt="../../_images/sphx_glr_plot_0_glm_reg_006.png" src="../../_images/sphx_glr_plot_0_glm_reg_006.png" /></a>
</figure>
<p>By centering the data and calculating the marginal effects, the model itself remains unchanged, but the interpretation of the marginal effects may change significantly, as we are essentially comparing the contribution of each feature to the average population, rather than the individual sample values. For instance, <code class="docutils literal notranslate"><span class="pre">hum</span></code> has a large contribution to the prediction in the uncentered data. However, when the data is centered, its contribution becomes negligible. This is because the feature value of this sample is close to the population mean, and its contribution is now mainly captured by the intercept term.</p>
</section>
</section>
<section id="data-dependent-interpretation">
<h2><span class="section-number">5.1.4. </span>Data Dependent Interpretation<a class="headerlink" href="#data-dependent-interpretation" title="Permalink to this heading">¶</a></h2>
<p>The model interpretation can be affected by the data distribution. For example, the feature importance of a feature can be very different for different data sets. To address this issue, we can use the <code class="docutils literal notranslate"><span class="pre">use_test</span></code> parameter to switch between training and testing data for interpretation. For example, the following command shows the feature importance of the GLM model on the testing data.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_interpret</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;global_fi&quot;</span><span class="p">,</span> <span class="n">use_test</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/3_models/plot_0_glm_reg.html"><img alt="../../_images/sphx_glr_plot_0_glm_reg_007.png" src="../../_images/sphx_glr_plot_0_glm_reg_007.png" /></a>
</figure>
<p>For local interpretation, the <code class="docutils literal notranslate"><span class="pre">sample_id</span></code> parameter is used to select the sample in the training set. If you would like to interpret samples in the testing set, just set <code class="docutils literal notranslate"><span class="pre">use_test</span></code> to True. Then, the <code class="docutils literal notranslate"><span class="pre">sample_id</span></code> stands for the index of test set data.</p>
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="o">.</span><span class="n">model_interpret</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;GLM&quot;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="s2">&quot;local_fi&quot;</span><span class="p">,</span> <span class="n">sample_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">use_test</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">original_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<figure class="align-left">
<a class="reference external image-reference" href="../../auto_examples/3_models/plot_0_glm_reg.html"><img alt="../../_images/sphx_glr_plot_0_glm_reg_008.png" src="../../_images/sphx_glr_plot_0_glm_reg_008.png" /></a>
</figure>
</section>
<section id="examples">
<h2><span class="section-number">5.1.5. </span>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h2>
<aside class="topic">
<p class="topic-title">Example 1: Bike Sharing</p>
<blockquote>
<div><p>The first example below demonstrates how to use PiML with its high-code APIs for developing machine learning models for the BikeSharing data from the UCI repository, which consists of 17,389 samples of hourly counts of rental bikes in Capital bikeshare system; see details. The response <code class="docutils literal notranslate"><span class="pre">cnt</span></code> (hourly bike rental counts) is continuous and it is a regression problem.</p>
</div></blockquote>
<ul class="simple">
<li><p><a class="reference internal" href="../../auto_examples/3_models/plot_0_glm_reg.html#sphx-glr-auto-examples-3-models-plot-0-glm-reg-py"><span class="std std-ref">GLM Linear Regression (Bike Sharing)</span></a></p></li>
</ul>
</aside>
<aside class="topic">
<p class="topic-title">Example 2: Taiwan Credit</p>
<blockquote>
<div><p>The second example below demonstrates how to use PiML’s high-code APIs for the TaiwanCredit dataset from the UCI repository. This dataset comprises the credit card details of 30,000 clients in Taiwan from April 2005 to September 2005, and more information can be found on the TaiwanCreditData website. The data can be loaded directly into PiML, although it requires some preprocessing. The FlagDefault variable serves as the response for this classification problem.</p>
</div></blockquote>
<ul class="simple">
<li><p><a class="reference internal" href="../../auto_examples/3_models/plot_0_glm_cls.html#sphx-glr-auto-examples-3-models-plot-0-glm-cls-py"><span class="std std-ref">GLM Logistic Regression (Taiwan Credit)</span></a></p></li>
</ul>
</aside>
<aside class="topic">
<p class="topic-title">References</p>
<div role="list" class="citation-list">
<div class="citation" id="mccullagh1989" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">McCullagh1989</a><span class="fn-bracket">]</span></span>
<p>Peter McCullagh, John Nelder (1989). Generalized linear models, Chapman and Hall - second edition.</p>
</div>
</div>
</aside>
</section>
</section>


        </div>
      <div class="container">
        <footer class="sk-content-footer">
              &copy; Copyright 2022-, PiML-Toolbox authors.
            <a href="../../_sources/guides/models/glm.rst.txt" rel="nofollow">Show this page source</a>
        </footer>
      </div>
    </div>
  </div>


  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
      
      const tooltipTriggerList = document.querySelectorAll('[data-bs-toggle="tooltip"]')
      const tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl))
  </script>

<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    

<script src="_static/js/vendor/bootstrap.min.js"></script>

</body>
</html>