<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>PiML Toolbox</title>
  

  
  <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/plot_directive.css" type="text/css" />
  <link rel="stylesheet" href="../_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="../_static/thebelab.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
<script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>

</head>

<body class="wy-body-for-nav">

  


<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../index.html">
        <img
          class="sk-brand-img"
          src="../_static/piml-logo.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../install.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../user_guide.html">User Guide</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../modules/classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../auto_examples/index.html">Examples</a>
        </li>
     </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Go" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
  <div class="d-flex" id="sk-doc-wrapper">
      <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
      <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Toggle Menu</label>
      <div id="sk-sidebar-wrapper" class="border-right">
        <div class="sk-sidebar-toc-wrapper">
          <div class="sk-sidebar-toc-logo">
            <a href="../index.html">
              <img
                class="sk-brand-img"
                src="../_static/piml-logo.png"
                alt="logo"/>
            </a>
          </div>
          <!--div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
              <a href="../user_guide.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="User Guide">Prev</a><a href="../user_guide.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="User Guide">Up</a>
              <a href="data.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="2. Data Pipeline">Next</a>
          </div-->
              <div class="sk-sidebar-toc">
              
                <ul>
                
                
                
                
                
                
                <li>
                  <a href="../user_guide.html" class="sk-toc-active">User Guide</a>
                </li>
                <ul>
                
                  <li>
                    <a href="" class="sk-toc-active">1. Introduction</a>
                    
                  </li>
                
                  <li>
                    <a href="data.html" class="">2. Data Pipeline</a>
                    
                  </li>
                
                  <li>
                    <a href="train.html" class="">3. Model Train and Tune</a>
                    
                  </li>
                
                  <li>
                    <a href="explain.html" class="">4. Post-hoc Explainability</a>
                    
                  </li>
                
                  <li>
                    <a href="models.html" class="">5. Interpretable Models</a>
                    
                  </li>
                
                  <li>
                    <a href="testing.html" class="">6. Diagnostic Suite</a>
                    
                  </li>
                
                  <li>
                    <a href="compare.html" class="">7. Model Comparison</a>
                    
                  </li>
                
                  <li>
                    <a href="cases.html" class="">8. Case Studies</a>
                    
                  </li>
                
                </ul>
                
                
                
                
                </ul>
              </div>
        </div>
      </div>
      <div id="sk-page-content-wrapper">
        <div class="sk-page-content container-fluid body px-md-3" role="main">
          
  <style type="text/css">
  div.body div.toctree-wrapper ul {
      padding-left: 0;
  }

  div.body li.toctree-l1 {
      padding: 0 0 0.5em 0;
      list-style-type: none;
      font-size: 150%;
      font-weight: bold;
  }

  div.body li.toctree-l2 {
      font-size: 70%;
      list-style-type: square;
      font-weight: normal;
      margin-left: 40px;
  }

  div.body li.toctree-l3 {
      font-size: 85%;
      list-style-type: circle;
      font-weight: normal;
      margin-left: 40px;
  }

  div.body li.toctree-l4 {
      margin-left: 40px;
  }

</style><section id="introduction">
<h1><span class="section-number">1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h1>
<p>PiML (read <span class="math notranslate nohighlight">\(\pi\)</span>-ML, /`pai·`em·`el/) is an integrated and open-access Python toolbox for interpretable machine learning model development and model diagnostics. It is designed with machine learning workflows in both low-code and high-code modes, including data pipeline, model training, model interpretation and explanation, and model diagnostics and comparison. The toolbox supports a growing list of interpretable models (e.g., GAM, GAMI-Net, XGB2) with inherent local and/or global interpretability. It also supports model-agnostic explainability tools (e.g., PFI, PDP, LIME, SHAP) and a powerful suite of model-agnostic diagnostics (e.g., weakness, uncertainty, robustness, fairness). Integration of PiML models and tests to existing MLOps platforms for quality assurance are enabled by flexible high-code APIs. Furthermore, PiML toolbox comes with a comprehensive user guide and hands-on examples, including the applications for model development and validation in banking. The project is available at <a class="reference external" href="https://github.com/SelfExplainML/PiML-Toolbox">https://github.com/SelfExplainML/PiML-Toolbox</a>.</p>
<section id="id1">
<h2><span class="section-number">1.1. </span>Introduction<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<p>Supervised machine learning has being increasingly used in domains where decision making can have significant consequences. However, the lack of interpretability of many machine learning models makes it difficult to understand and trust the model-based decisions. This leads to growing interest in interpretable machine learning and model diagnostics. There emerge algorithms and packages for model-agnostic explainability, including the <strong>inspection</strong> module (including permutation feature importance, partial dependence) in <strong>scikit-learn</strong> <a class="reference internal" href="#pedregosa2011" id="id2"><span>[Pedregosa2011]</span></a> and various others, e.g., <a class="reference internal" href="#kokhlikyan2020" id="id3"><span>[Kokhlikyan2020]</span></a>, <a class="reference internal" href="#klaise2021" id="id4"><span>[Klaise2021]</span></a>, <a class="reference internal" href="#baniecki2021" id="id5"><span>[Baniecki2021]</span></a>, <a class="reference internal" href="#li2022" id="id6"><span>[Li2022]</span></a>.</p>
<p>Post-hoc explainability tools are useful for black-box models, but they are known to have general pitfalls <a class="reference internal" href="#rudin2019" id="id7"><span>[Rudin2019]</span></a>, <a class="reference internal" href="#molnar2020" id="id8"><span>[Molnar2020]</span></a>. Inherently interpretable models are suggested for machine learning model development <a class="reference internal" href="#yang2021a" id="id9"><span>[Yang2021a]</span></a>, <a class="reference internal" href="#yang2021b" id="id10"><span>[Yang2021b]</span></a>, <a class="reference internal" href="#sudjianto2020" id="id11"><span>[Sudjianto2020]</span></a>. The <strong>InterpretML</strong> package <a class="reference internal" href="#nori2013" id="id12"><span>[Nori2013]</span></a> by Microsoft Research is such a package of promoting the use of inherently interpretable models, in particular their explainable boosting machine (EBM) based on the GA2M model <a class="reference internal" href="#lou2013" id="id13"><span>[Lou2013]</span></a>. One may also refer to <a class="reference internal" href="#sudjianto2021" id="id14"><span>[Sudjianto2021]</span></a> for discussion about how to design inherently interpretable machine learning models.</p>
<p>In the meantime, model diagnostic tools become increasingly important for model validation and outcome testing. New tools and platforms are developed for model weakness detection and error analysis, e.g., <a class="reference internal" href="#chung2019" id="id15"><span>[Chung2019]</span></a>, PyCaret package, TensorFlow model analysis, FINRA’s model validation toolkit, and Microsoft’s responsible AI toolbox. They can be used for arbitrary pre-trained models, in the same way as the post-hoc explainability tools. Such type of model diagnostics or validation is sometimes referred to as black-box testing, and there is an increasing demand of diagnostic tests for quality assurance of machine learning models.</p>
<p>It is our goal to design an integrated Python toolbox for interpretable machine learning, for both model development and model diagnostics. This is particularly needed for model risk management in banking, where it is a routine exercise to run model validation including evaluation of model conceptual soundness and outcome testing from various angles. An inherently interpretable machine learning model tends to be more conceptually sound, while it is subject to model diagnostics in terms of accuracy, weakness detection, fairness, uncertainty, robustness and resilience. The PiML toolbox we develop is such a unique Python tool that supports not only a growing list of interpretable models, but also an enhanced suite of multiple diagnostic tests. It has been adopted by multiple banks since its first launch on May 4, 2022.</p>
</section>
<section id="toolbox-design">
<h2><span class="section-number">1.2. </span>Toolbox Design<a class="headerlink" href="#toolbox-design" title="Permalink to this heading">¶</a></h2>
<p>PiML toolbox is designed to support machine learning workflows by both low-code interface and high-code APIs; see Figure below for the overall design.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/PiML_Workflow.png"><img alt="Design of PiML workflows with low-code interface and high-code APIs" src="../_images/PiML_Workflow.png" style="width: 600px;" /></a>
</figure>
<ul class="simple">
<li><p><strong>Low-code panels</strong>: interactive widgets or dashboards are developed for Jupyter notebook or Jupyter lab users. A minimum level of Python coding is required. The data pipeline consists of convenient <code class="docutils literal notranslate"><span class="pre">exp.data_load()</span></code>, <code class="docutils literal notranslate"><span class="pre">exp.data_summary()</span></code>, <code class="docutils literal notranslate"><span class="pre">exp.eda()</span></code>, <code class="docutils literal notranslate"><span class="pre">exp.data_quality()</span></code>, <code class="docutils literal notranslate"><span class="pre">exp.feature_select()</span></code>, <code class="docutils literal notranslate"><span class="pre">exp.data_prepare()</span></code>, each calling an interactive panel with choices of parameterization and actions.</p></li>
<li><p><strong>High-code APIs</strong>: each low-code panel can be also executed through one or more Python functions with manually specified  options and parameters. Such high-code APIs are flexible to be called both in Jupyter notebook cells and by Terminal command lines. High-code APIs usually provide more options than their default use in low-code panels. End-to-end pipeline automation can be enabled with appropriate high-code settings.</p></li>
<li><p><strong>Existing models</strong>: a pre-trained model can be loaded to PiML experimentation through pipeline registration. It is mandatory to include both training and testing datasets, in order for the model to take the full advantage of PiML explanation and diagnostic capabilities. It can be an arbitrary model in supervised learning settings, including regression and binary classification.</p></li>
</ul>
<p>For PiML-trained models by either low-code interface or high-code APIs, there are four follow-up actions to be executed:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_interpret()</span></code>: this unified API works only for inherently interpretable models (a.k.a., glass models) to be discussed in <a class="reference external" href="introduction.html#interpretable-models">section_3</a>. It provides model-specific interpretation in both global and local ways. For example, a linear model is interpreted locally through model coefficients or marginal effects, while a decision tree is interpreted locally through the tree path.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_explain()</span></code>: this unified API works for arbitrary models including black-box models and glass-box models. It provides post-hoc global explainability through permutation feature importance (PFI) and partial dependence plot (PDP) through <code class="docutils literal notranslate"><span class="pre">sklearn.inspect</span></code> module, accumulated local effects <a class="reference internal" href="#apley2016" id="id16"><span>[Apley2016]</span></a>, and post-hoc local explainability through LIME <a class="reference internal" href="#ribeiro2016" id="id17"><span>[Ribeiro2016]</span></a> and SHAP <a class="reference internal" href="#lundberg2017" id="id18"><span>[Lundberg2017]</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_diagnose()</span></code>: this unified API works for arbitrary models and performs model diagnostics to be discussed in <a class="reference external" href="introduction.html#diagnostic-suite">section_4</a>. It is designed to cover standardized general-purpose tests based on model data and predictions, i.e., model-agnostic tests. There is no need to access the model internals.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_compare()</span></code>: this unified API is to compare two or three models at the same time, in terms of model performance and other diagnostic aspects. By inspecting the dashboard of graphical plots, one can easily rank models under comparison.</p></li>
</ul>
<p>For registered models that are not trained from PiML, they are automatically treated as black-box models, even though such a model may be inherently interpretable (e.g., linear regression model). This is due to simplification of pipeline registration, where only the model prediction method is considered. For these models, <code class="docutils literal notranslate"><span class="pre">model_interpret()</span></code> is not valid, while the other three unified APIs are fully functional.</p>
<p>Regarding PiML high-code APIs, it is worthwhile to mention that these APIs are flexible enough for integration into existing MLOps platforms. After PiML installation to MLOps backend, the high-code APIs can be called not only to train interpretable models, but also to perform arbitrary model testing for quality assurance.</p>
</section>
<section id="interpretable-models">
<h2><span class="section-number">1.3. </span>Interpretable Models<a class="headerlink" href="#interpretable-models" title="Permalink to this heading">¶</a></h2>
<p>PiML supports a growing list of inherently interpretable models. For simplicity, we only list the models and the references. The following list of interpretable models are included PiML toolbox V0.5 (latest update: May 4, 2023).</p>
<p>PiML supports a growing list of inherently interpretable models. For simplicity, we only list the models and the references. The following list of interpretable models are included PiML toolbox V0.5 (latest update: May 4, 2023).</p>
<ul class="simple">
<li><p><strong>GLM</strong>: Linear/logistic regression with <span class="math notranslate nohighlight">\(\ell_1\)</span> and/or <span class="math notranslate nohighlight">\(\ell_2\)</span> regularization <a class="reference internal" href="#hastie2015" id="id19"><span>[Hastie2015]</span></a></p></li>
<li><p><strong>GAM</strong>: Generalized additive models using B-splines <a class="reference internal" href="#serven2018" id="id20"><span>[Serven2018]</span></a></p></li>
<li><p><strong>Tree</strong>: Decision tree for classification and regression <a class="reference internal" href="#pedregosa2011" id="id21"><span>[Pedregosa2011]</span></a></p></li>
<li><p><strong>FIGS</strong>: Fast interpretable greedy-tree sums <a class="reference internal" href="#tan2022" id="id22"><span>[Tan2022]</span></a></p></li>
<li><p><strong>XGB1</strong>: Extreme gradient boosted trees of depth 1, using optimal binning <a class="reference internal" href="#chen2015" id="id23"><span>[Chen2015]</span></a>, <a class="reference internal" href="#guillermo2020" id="id24"><span>[Guillermo2020]</span></a></p></li>
<li><p><strong>XGB2</strong>: Extreme gradient boosted trees of depth 2, with purified effects <a class="reference internal" href="#chen2015" id="id25"><span>[Chen2015]</span></a>, <a class="reference internal" href="#lengerich2020" id="id26"><span>[Lengerich2020]</span></a></p></li>
<li><p><strong>EBM</strong>: Explainable boosting machine <a class="reference internal" href="#lou2013" id="id27"><span>[Lou2013]</span></a>, <a class="reference internal" href="#nori2013" id="id28"><span>[Nori2013]</span></a></p></li>
<li><p><strong>GAMI-Net</strong>: Generalized additive model with structured interactions <a class="reference internal" href="#yang2021b" id="id29"><span>[Yang2021b]</span></a></p></li>
<li><p><strong>ReLU-DNN</strong>: Deep ReLU networks using Aletheia unwrapper and sparsification <a class="reference internal" href="#sudjianto2020" id="id30"><span>[Sudjianto2020]</span></a></p></li>
</ul>
</section>
<section id="diagnostic-suite">
<h2><span class="section-number">1.4. </span>Diagnostic Suite<a class="headerlink" href="#diagnostic-suite" title="Permalink to this heading">¶</a></h2>
<p>PiML comes with a continuously enhanced suite of diagnostic tests for arbitrary supervised machine learning models under regression and binary classification settings. Below is a list of the supported general-purpose tests with brief descriptions.</p>
<ul class="simple">
<li><p><strong>Accuracy</strong>: popular metrics like MSE, MAE for regression tasks and ACC, AUC, Recall, Precision, F1-score for binary classification tasks.</p></li>
<li><p><strong>WeakSpot</strong>: identification of weak regions with high magnitude of residuals by 1D and 2D slicing techniques.</p></li>
<li><p><strong>Overfit/Underfit</strong>: identification of overfitting/underfitting regions according to train-test performance gap, also by 1D and 2D slicing techniques.</p></li>
<li><p><strong>Reliability</strong>: quantification of prediction uncertainty by split conformal prediction and slicing techniques.</p></li>
<li><p><strong>Robustness</strong>: evaluation of performance degradation under different sizes of covariate noise perturbation <a class="reference internal" href="#cui2023" id="id31"><span>[Cui2023]</span></a>.</p></li>
<li><p><strong>Resilience</strong>: evaluation of performance degradation under different out-of-distribution scenarios.</p></li>
<li><p><strong>Fairness</strong>: disparity test, segmented analysis and model de-bias through binning and thresholding techniques.</p></li>
</ul>
</section>
<section id="future-plan">
<h2><span class="section-number">1.5. </span>Future Plan<a class="headerlink" href="#future-plan" title="Permalink to this heading">¶</a></h2>
<p>PiML toolbox is our new initiative of integrating state-of-the-art methods in interpretable machine learning and model diagnostics. It provides convenient user interfaces and flexible APIs for easy use  of model interpretation, explanation, testing and comparison. Our future plan is to continuously improve the user experience, add new interpretable models, and expand the diagnostic suite. It is also our plan to enhance PiML experimentation with tracking and reporting.</p>
<aside class="topic">
<p class="topic-title">References</p>
<div role="list" class="citation-list">
<div class="citation" id="pedregosa2011" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Pedregosa2011<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id21">2</a>)</span>
<p>Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay (2011).
<a class="reference external" href="https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf">Scikit-learn: Machine learning in Python</a>,
Journal of machine Learning research, 12, 2825-2830.</p>
</div>
<div class="citation" id="kokhlikyan2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Kokhlikyan2020</a><span class="fn-bracket">]</span></span>
<p>Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds,Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, Orion Reblitz-Richardson (2020).
<a class="reference external" href="https://arxiv.org/pdf/2009.07896.pdf">Captum: A unified and generic model interpretability library for pytorch</a>,
arXiv preprint arXiv:2009.07896.</p>
</div>
<div class="citation" id="klaise2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Klaise2021</a><span class="fn-bracket">]</span></span>
<p>Janis Klaise, Arnaud Van Looveren, Giovanni Vacanti, Alexandru Coca (2021).
<a class="reference external" href="https://www.jmlr.org/papers/volume22/21-0017/21-0017.pdf">Alibi explain: Algorithms for explaining machine learning models</a>,
Journal of Machine Learning Research, 22(1), 8194-8200.</p>
</div>
<div class="citation" id="baniecki2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">Baniecki2021</a><span class="fn-bracket">]</span></span>
<p>Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, Jakub Wisniewski, Przemyslaw Biecek (2021).
<a class="reference external" href="https://www.jmlr.org/papers/volume22/20-1473/20-1473.pdf">Dalex: responsible machine learning with interactive explainability and fairness in python</a>,
Journal of Machine Learning Research, 22(1), 9759-9765.</p>
</div>
<div class="citation" id="li2022" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">Li2022</a><span class="fn-bracket">]</span></span>
<p>Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Zeyu Chen, Dejing Dou (2022).
<a class="reference external" href="https://www.jmlr.org/papers/volume23/21-0738/21-0738.pdf">InterpretDL: Explaining Deep Models in PaddlePaddle</a>,
Journal of Machine Learning Research, 23(197), 1-6.</p>
</div>
<div class="citation" id="rudin2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">Rudin2019</a><span class="fn-bracket">]</span></span>
<p>Cynthia Rudin (2019).
<a class="reference external" href="https://arxiv.org/pdf/1811.10154.pdf">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</a>,
Nature Machine ntelligence, 1(5), 206-215.</p>
</div>
<div class="citation" id="molnar2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">Molnar2020</a><span class="fn-bracket">]</span></span>
<p>Christoph Molnar, Gunnar König, Julia Herbinger, Timo Freiesleben, Susanne Dandl, Christian A. Scholbeck, Giuseppe Casalicchio, Moritz Grosse-Wentrup, Bernd Bischl (2022, April).
<a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-031-04083-2_4">General pitfalls of model-agnostic interpretation methods for machine learning models</a>,
In xxAI-Beyond Explainable AI: International Workshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers (pp. 39-68). Cham: Springer International Publishing.</p>
</div>
<div class="citation" id="nori2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Nori2013<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id12">1</a>,<a role="doc-backlink" href="#id28">2</a>)</span>
<p>Harsha Nori, Samuel Jenkins, Paul Koch, Rich Caruana (2019). <a class="reference external" href="https://arxiv.org/pdf/1909.09223.pdf">InterpretML A Unified Framework for Machine Learning Interpretability</a>, arXiv preprint arXiv:1909.09223.</p>
</div>
<div class="citation" id="lou2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Lou2013<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id13">1</a>,<a role="doc-backlink" href="#id27">2</a>)</span>
<p>Yin Lou, Rich Caruana, Johannes Gehrke, Giles Hooker (2013).
<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/2487575.2487579">Accurate intelligible models with pairwise interactions</a>,
In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 623-631).</p>
</div>
<div class="citation" id="sudjianto2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">Sudjianto2021</a><span class="fn-bracket">]</span></span>
<p>Agus Sudjianto, Aijun Zhang (2021).
<a class="reference external" href="https://arxiv.org/pdf/2111.01743.pdf">Designing Inherently Interpretable Machine Learning Models</a>,
arXiv preprint arXiv:2111.01743.</p>
</div>
<div class="citation" id="chung2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">Chung2019</a><span class="fn-bracket">]</span></span>
<p>Yeounoh Chung, Tim Kraska, Neoklis Polyzotis, Ki Hyun Tae, Steven Euijong Whang (2019, April).
<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/8731353">Slice finder: Automated data slicing for model validation</a>,
In 2019 IEEE 35th International Conference on Data Engineering (ICDE) (pp. 1550-1553). IEEE.</p>
</div>
<div class="citation" id="apley2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">Apley2016</a><span class="fn-bracket">]</span></span>
<p>Daniel W. Apley, Jingyu Zhu (2016).
<a class="reference external" href="https://academic.oup.com/jrsssb/article/82/4/1059/7056085">Visualizing the effects of predictor variables in black box supervised learning models</a>,
Journal of the Royal Statistical Society Series B: Statistical Methodology 82.4 (2020): 1059-1086.</p>
</div>
<div class="citation" id="ribeiro2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">Ribeiro2016</a><span class="fn-bracket">]</span></span>
<p>Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin (2016).
<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/2939672.2939778">Why should i trust you?” Explaining the predictions of any classifier</a>,
Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining.</p>
</div>
<div class="citation" id="lundberg2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">Lundberg2017</a><span class="fn-bracket">]</span></span>
<p>Scott Lundberg, Su-In Lee (2017).
<a class="reference external" href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">A unified approach to interpreting model predictions</a>,
Advances in neural information processing systems 30.</p>
</div>
<div class="citation" id="hastie2015" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">Hastie2015</a><span class="fn-bracket">]</span></span>
<p>Trevor Hastie, Robert Tibshirani, Martin Wainwright (2015).
<a class="reference external" href="https://hastie.su.domains/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf">Statistical learning with sparsity: the lasso and generalizations</a>,
CRC press.</p>
</div>
<div class="citation" id="friedman2001" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Friedman2001<span class="fn-bracket">]</span></span>
<p>Jerome H. Friedman (2001).
<a class="reference external" href="https://www.jstor.org/stable/2699986">Greedy function approximation: a gradient boosting machine</a>,
The Annals of Statistics. 29(5): 1189-1232.</p>
</div>
<div class="citation" id="serven2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">Serven2018</a><span class="fn-bracket">]</span></span>
<p>Daniel Servén, Charlie Brummitt (2018).
<a class="reference external" href="https://zenodo.org/record/1476122#.ZFEIeXZBzEY">pyGAM: Generalized Additive Models in Python</a>,
Zenodo. DOI: 10.5281/zenodo.1208723</p>
</div>
<div class="citation" id="tan2022" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">Tan2022</a><span class="fn-bracket">]</span></span>
<p>Yan Shuo Tan, Chandan Singh, Keyan Nasseri, Abhineet Agarwal, Bin Yu (2022).
<a class="reference external" href="https://arxiv.org/pdf/2201.11931.pdf">Fast interpretable greedy-tree sums (FIGS)</a>,
arXiv preprint arXiv:2201.11931.</p>
</div>
<div class="citation" id="lengerich2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">Lengerich2020</a><span class="fn-bracket">]</span></span>
<p>Benjamin Lengerich, Sarah Tan, Chun-Hao Chang, Giles Hooker, Rich Caruana (2020, June).
<a class="reference external" href="http://proceedings.mlr.press/v108/lengerich20a.html">Purifying interaction effects with the functional anova: An efficient algorithm for recovering identifiable additive models</a>,
In International Conference on Artificial Intelligence and Statistics (pp. 2402-2412). PMLR.</p>
</div>
<div class="citation" id="chen2015" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Chen2015<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id23">1</a>,<a role="doc-backlink" href="#id25">2</a>)</span>
<p>Tianqi Chen, Tong He (2015).
<a class="reference external" href="https://cran.microsoft.com/snapshot/2017-12-11/web/packages/xgboost/vignettes/xgboost.pdf">Xgboost: extreme gradient boosting</a>, R package version 0.4-2, 1(4), 1-4.</p>
</div>
<div class="citation" id="sudjianto2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Sudjianto2020<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id30">2</a>)</span>
<p>Agus Sudjianto, William Knauth, Rahul Singh, Zebin Yang, Aijun Zhang. (2020)
<a class="reference external" href="https://arxiv.org/pdf/2011.04041.pdf">Unwrapping the black box of deep ReLU networks: interpretability, diagnostics, and simplification</a>,
arXiv preprint arXiv:2011.04041.</p>
</div>
<div class="citation" id="cui2023" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id31">Cui2023</a><span class="fn-bracket">]</span></span>
<p>Shijie Cui, Agus Sudjianto, Aijun Zhang, Runze Li (2023).
<a class="reference external" href="https://arxiv.org/pdf/2304.13761.pdf">Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization</a>,
arXiv preprint arXiv:2304.13761.</p>
</div>
<div class="citation" id="yang2021a" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">Yang2021a</a><span class="fn-bracket">]</span></span>
<p>Zebin Yang, Aijun Zhang, Agus Sudjianto (2021).
<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9149804">Enhancing explainability of neural networks through architecture constraints</a>,
IEEE Transactions on Neural Networks and Learning Systems, 32(6), 2610-2621.</p>
</div>
<div class="citation" id="yang2021b" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Yang2021b<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id10">1</a>,<a role="doc-backlink" href="#id29">2</a>)</span>
<p>Zebin Yang, Aijun Zhang, Agus Sudjianto (2021).
<a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321003484">GAMI-Net: An explainable neural network based on generalized additive models with structured interactions</a>,
Pattern Recognition, 120, 108192.</p>
</div>
<div class="citation" id="guillermo2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">Guillermo2020</a><span class="fn-bracket">]</span></span>
<p>Navas-Palencia, Guillermo (2020). <a class="reference external" href="https://arxiv.org/pdf/2001.08025.pdf">Optimal binning: mathematical programming formulation.</a>, arXiv preprint arXiv:2001.08025.</p>
</div>
</div>
</aside>
</section>
</section>


        </div>
      <div class="container">
        <footer class="sk-content-footer">
              &copy; Copyright 2022-, PiML-Toolbox authors.
            <a href="../_sources/guides/introduction.rst.txt" rel="nofollow">Show this page source</a>
        </footer>
      </div>
    </div>
  </div>


  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
      
      const tooltipTriggerList = document.querySelectorAll('[data-bs-toggle="tooltip"]')
      const tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl))
  </script>

<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    

<script src="_static/js/vendor/bootstrap.min.js"></script>

</body>
</html>