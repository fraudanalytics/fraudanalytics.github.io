{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Register PySpark Models\nHere we show how to write a wrapper for PySpark models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For demonstration, we first fit a Decision tree model using SimuCredit data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nos.environ[\"PYSPARK_PYTHON\"] = \"python\"\n\nimport pandas as pd\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer\n\ndata = pd.read_csv('https://github.com/SelfExplainML/PiML-Toolbox/blob/main/datasets/SimuCredit.csv?raw=true')\nfeature_names = ['Mortgage', 'Balance', 'Amount Past Due', 'Credit Inquiry', 'Open Trade', 'Delinquency', 'Utilization']\ntarget_name = 'Approved'\n\nspark = SparkSession.builder.appName(\"SimuCredit-Spark-Demo\").getOrCreate()\nspark_df = spark.createDataFrame(data)\n\nfeature_assembler = VectorAssembler(inputCols=feature_names, outputCol=\"features\")\nlabel_stringIdx = StringIndexer(inputCol=target_name, outputCol='label')\n\npipeline = Pipeline(stages=[feature_assembler, label_stringIdx])\npipelineModel = pipeline.fit(spark_df)\ntrain_data, test_data = pipelineModel.transform(spark_df).randomSplit([0.8, 0.2], seed=2024)\n\ndt = DecisionTreeClassifier(featuresCol='features', labelCol='label', seed=2024)\nmodel = dt.fit(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the wrapper functions of predict and predict_proba.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def predict_func(X):\n\n    spark_df = pipelineModel.transform(spark.createDataFrame(pd.DataFrame(X, columns=feature_names)))\n    pred = model.transform(spark_df).select('prediction').toPandas().values.astype(float).ravel()\n    return pred\n\ndef predict_proba_func(X):\n    \n    spark_df = pipelineModel.transform(spark.createDataFrame(pd.DataFrame(X, columns=feature_names)))\n    predictions = model.transform(spark_df).select('probability').toPandas()\n    proba = predictions.explode('probability').values.reshape((-1, 2)).astype(float)\n    return proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Register the fitted model into PiML (please make sure the datasets of different pipelines are the same)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml import Experiment\nexp = Experiment(highcode_only=True)\npipeline = exp.make_pipeline(predict_func=predict_func,\n                             predict_proba_func=predict_proba_func,\n                             task_type=\"classification\",\n                             train_x=train_data.select(feature_names).toPandas().values,\n                             train_y=train_data.select(target_name).toPandas().values,\n                             test_x=test_data.select(feature_names).toPandas().values,\n                             test_y=test_data.select(target_name).toPandas().values,\n                             feature_names=feature_names,\n                             target_name=target_name)\nexp.register(pipeline, \"Spark-GBT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check model performance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exp.model_diagnose(model=\"Spark-GBT\", show=\"accuracy_table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run validataion tests\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exp.model_explain(model=\"Spark-GBT\", show=\"pdp\", uni_feature=\"Balance\", sample_size=1000, figsize=(5, 4))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}