.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../../includes/big_toc_css.rst

======================================
Data Load
======================================
This section introduces the data loader module of PiML. Data loader is usually the first step of the whole experiment, and it supports choosing a built-in dataset or external dataset to start your experiment.


Built-in Dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

 There are several datasets that are already uploaded into PiML. These are: 

   - 'CoCircles': Gaussian data with a spherical decision boundary for binary classification, generated via Scikit-Learn.
   - 'Friedman': 'Friedman #1' regression problem, generated via Scikit-Learn.
   - 'BikeSharing': Refer to https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset.
   - 'TaiwanCredit': Refer to https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients.
   - 'CaliforniaHousing_raw': Refer to https://developers.google.com/machine-learning/crash-course/california-housing-data-description.
   - 'CaliforniaHousing_trim1': 'CaliforniaHousing_raw' dataset with the feature 'AveOccup' trimmed by upper threshold 5.
   - 'CaliforniaHousing_trim2': 'CaliforniaHousing_raw' dataset with the features 'AveRooms', 'AveBedrms', 'Population', and 'AveOccup' trimmed by upper threshold quantile (0.98).
   - 'SimuCredit': A credit simulation data for fairness testing.
   - 'SolasSimu1': A simulated dataset, modified from the 'Friedman #1' regression problem. The covariates used for modeling are 'Segment', 'x1', 'x2', ..., and 'x5', the response 'Label' is binary and it is a classification problem. The rest variables are demographic variables used for testing fairness. The data is contributed by Solas-AI (https://github.com/SolasAI/solas-ai-disparity).
   - 'SolasHMDA': A preprocessed sample of the 2018 Home Mortgage Disclosure Act (HMDA) data. The HMDA dataset includes information about nearly every home mortgage application in the United States.

You could load these datasets using the code below, where `data=" "` indicates the dataset to be included. For example,

.. jupyter-input::

    exp.data_loader(data="CoCircles")

.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>X0</th>               
          <th>X1</th>
          <th>target</th> 
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>    
          <td>-0.783526</td>
          <td>0.502161</td>
          <td>0.0</td>                      
        </tr>
        <tr>
          <th>1</th>    
          <td>0.297809</td>
          <td>0.658405</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>2</th>    
          <td>0.468272</td>
          <td>0.500653</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th></th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>1997</th>
          <td>-0.542930</td>      
          <td>-0.583517</td>
          <td>1.0</td>                        
        </tr>
        <tr>
          <th>1998</th>
          <td>-0.871481</td>
          <td>-0.491301</td>      
          <td>0.0</td>                      
        </tr>
        <tr>
          <th>1999</th>     
          <td>-0.323963</td>
          <td>-0.719150</td>
          <td>0.0</td>                                
        </tr>
      </tbody>
    </table>
    </div>
    </div>
  

External Dataset (csv files)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
There are two ways of loading csv files in PiML.

- In low-code mode, you could just click the "upload new data" button to upload your data.

- In high-code mode, you can use pandas to wrap your data and input it to the data loader. For example:

.. jupyter-input::

    data = pd.read_csv('https://github.com/SelfExplainML/PiML-Toolbox/blob/main/datasets/BikeSharing.csv?raw=true')
    exp.data_loader(data=data)

.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>season</th>               
          <th>yr</th>
          <th>mnth</th>
          <th>hr</th>
          <th>holiday</th>
          <th>weekday</th>
          <th>workingday</th>
          <th>weathersit</th>
          <th>temp</th>
          <th>atemp</th>
          <th>hum</th>
          <th>windspeed</th>
          <th>cnt</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>1.0</td>
          <td>0.0</td>
          <td>1.0</td>                
          <td>0.0</td>
          <td>0.0</td>
          <td>6.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.24</td>
          <td>0.2879</td>
          <td>0.81</td>
          <td>0.0000</td>
          <td>16.0</td>                       
        </tr>
        <tr>
          <th>1</th>
          <td>1.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>6.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.22</td>
          <td>0.2727</td>
          <td>0.80</td>
          <td>0.0000</td>
          <td>40.0</td>
        </tr>
        <tr>
          <th>2</th>
          <td>1.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>2.0</td>
          <td>0.0</td>
          <td>6.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.22</td>
          <td>0.2727</td>
          <td>0.80</td>
          <td>0.0000</td>
          <td>32.0</td>
        </tr>
        <tr>
          <th></th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>17377</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>12.0</td>
          <td>21.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>0.26</td>
          <td>0.2576</td>
          <td>0.60</td>
          <td>0.1642</td>
          <td>90.0</td>                         
        </tr>
        <tr>
          <th>17378</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>12.0</td>
          <td>22.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>0.26</td>
          <td>0.2727</td>
          <td>0.56</td>
          <td>0.1343</td>
          <td>61.0</td>                       
        </tr>
        <tr>
          <th>17379</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>12.0</td>
          <td>23.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>0.26</td>
          <td>0.2727</td>
          <td>0.65</td>
          <td>0.1343</td>
          <td>49.0</td>                                 
        </tr>
      </tbody>
    </table>
    </div>
    </div>


External Dataset (Spark file)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Starting from PiML-0.6.0, we support to load data using spark backend. The data should be in the format of spark dataframe. For example, you could load the data from a parquet file using the following code:

.. jupyter-input::

    exp.data_loader(data="./myfile.parquet", spark=True, spark_sample_size=10000, spark_random_state=0)

.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
      <tr style="text-align: right;">
        <th></th>
        <th>Y</th>
        <th>X0</th>
        <th>X1</th>
        <th>X2</th>
        <th>X3</th>
        <th>X4</th>
        <th>X5</th>
        <th>X6</th>
        <th>X7</th>
        <th>X8</th>
        <th>X9</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th>0</th>
        <td>0.0</td>
        <td>-0.162726</td>
        <td>-0.989380</td>
        <td>-0.977290</td>
        <td>0.022444</td>
        <td>-0.833418</td>
        <td>-0.897849</td>
        <td>0.931033</td>
        <td>0.718005</td>
        <td>-0.695946</td>
        <td>-0.998672</td>
      </tr>
      <tr>
        <th>1</th>
        <td>1.0</td>
        <td>0.883336</td>
        <td>-0.443349</td>
        <td>-0.628205</td>
        <td>0.383016</td>
        <td>-0.782193</td>
        <td>-0.470701</td>
        <td>0.950189</td>
        <td>0.278926</td>
        <td>0.041356</td>
        <td>-0.204163</td>
      </tr>
      <tr>
        <th>2</th>
        <td>1.0</td>
        <td>0.549002</td>
        <td>-0.718085</td>
        <td>0.934676</td>
        <td>0.722246</td>
        <td>0.235314</td>
        <td>-0.914188</td>
        <td>0.401711</td>
        <td>0.826569</td>
        <td>0.049154</td>
        <td>-0.291550</td>
      </tr>
      <tr>
        <th>3</th>
        <td>0.0</td>
        <td>-0.759445</td>
        <td>0.509802</td>
        <td>0.770044</td>
        <td>-0.799497</td>
        <td>0.517969</td>
        <td>-0.965879</td>
        <td>0.934110</td>
        <td>0.230116</td>
        <td>0.104878</td>
        <td>-0.408100</td>
      </tr>
      <tr>
        <th>4</th>
        <td>1.0</td>
        <td>0.858583</td>
        <td>-0.468189</td>
        <td>0.656293</td>
        <td>0.970217</td>
        <td>0.566793</td>
        <td>0.037980</td>
        <td>-0.867851</td>
        <td>-0.055172</td>
        <td>-0.123488</td>
        <td>-0.594408</td>
      </tr>
      <tr>
        <th>...</th>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
      </tr>
      <tr>
        <th>2117</th>
        <td>1.0</td>
        <td>-0.325774</td>
        <td>-0.298574</td>
        <td>-0.984911</td>
        <td>-0.660747</td>
        <td>-0.894813</td>
        <td>0.971714</td>
        <td>-0.263716</td>
        <td>0.386797</td>
        <td>0.177803</td>
        <td>0.546942</td>
      </tr>
      <tr>
        <th>2118</th>
        <td>1.0</td>
        <td>0.631497</td>
        <td>-0.209968</td>
        <td>-0.060872</td>
        <td>-0.981552</td>
        <td>0.319807</td>
        <td>-0.552832</td>
        <td>-0.256842</td>
        <td>0.059649</td>
        <td>-0.120317</td>
        <td>0.194922</td>
      </tr>
      <tr>
        <th>2119</th>
        <td>1.0</td>
        <td>0.918874</td>
        <td>0.502420</td>
        <td>0.759211</td>
        <td>0.143963</td>
        <td>0.851615</td>
        <td>0.530987</td>
        <td>0.295923</td>
        <td>-0.576709</td>
        <td>-0.472256</td>
        <td>-0.470885</td>
      </tr>
      <tr>
        <th>2120</th>
        <td>1.0</td>
        <td>-0.505041</td>
        <td>-0.592865</td>
        <td>0.458442</td>
        <td>0.022174</td>
        <td>-0.396257</td>
        <td>0.430562</td>
        <td>0.394588</td>
        <td>0.286274</td>
        <td>0.493732</td>
        <td>-0.581601</td>
      </tr>
      <tr>
        <th>2121</th>
        <td>0.0</td>
        <td>0.304829</td>
        <td>0.028269</td>
        <td>0.903502</td>
        <td>-0.400436</td>
        <td>-0.932546</td>
        <td>-0.409703</td>
        <td>0.272170</td>
        <td>0.739227</td>
        <td>-0.744509</td>
        <td>-0.398449</td>
      </tr>
      </tbody>
    </table>
    </div>
    </div>

The argument `spark=True` tells the program that we would load data using spark backend, `data` denotes the file path, and `spark_sample_size` is expected sample size. By default, we do purly random subsampling, and the sample size will be transformed into the `frac` parameter in `spark`_. For example, if the original data has 100000 samples and the `spark_sample_size` is 10000, then the `frac` parameter will be set to 0.1. However, due to the working mechanism of spark, the actual sample size may be slightly different from the expected sample size.

.. _spark: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.sample.html


If you want to do stratified sampling, you could specify the stratified feature using `spark_sample_by_feature` parameter. For example, if you want to do stratified sampling using the feature 'Y', you could use the following code:

.. jupyter-input::

    exp.data_loader(data="./myfile.parquet", spark=True, spark_sample_size=10000,
                    spark_sample_by_feature='Y', spark_random_state=0)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
      <tr style="text-align: right;">
        <th></th>
        <th>Y</th>
        <th>X0</th>
        <th>X1</th>
        <th>X2</th>
        <th>X3</th>
        <th>X4</th>
        <th>X5</th>
        <th>X6</th>
        <th>X7</th>
        <th>X8</th>
        <th>X9</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th>0</th>
        <td>0.0</td>
        <td>-0.162726</td>
        <td>-0.989380</td>
        <td>-0.977290</td>
        <td>0.022444</td>
        <td>-0.833418</td>
        <td>-0.897849</td>
        <td>0.931033</td>
        <td>0.718005</td>
        <td>-0.695946</td>
        <td>-0.998672</td>
      </tr>
      <tr>
        <th>1</th>
        <td>1.0</td>
        <td>0.883336</td>
        <td>-0.443349</td>
        <td>-0.628205</td>
        <td>0.383016</td>
        <td>-0.782193</td>
        <td>-0.470701</td>
        <td>0.950189</td>
        <td>0.278926</td>
        <td>0.041356</td>
        <td>-0.204163</td>
      </tr>
      <tr>
        <th>2</th>
        <td>1.0</td>
        <td>0.549002</td>
        <td>-0.718085</td>
        <td>0.934676</td>
        <td>0.722246</td>
        <td>0.235314</td>
        <td>-0.914188</td>
        <td>0.401711</td>
        <td>0.826569</td>
        <td>0.049154</td>
        <td>-0.291550</td>
      </tr>
      <tr>
        <th>3</th>
        <td>0.0</td>
        <td>-0.759445</td>
        <td>0.509802</td>
        <td>0.770044</td>
        <td>-0.799497</td>
        <td>0.517969</td>
        <td>-0.965879</td>
        <td>0.934110</td>
        <td>0.230116</td>
        <td>0.104878</td>
        <td>-0.408100</td>
      </tr>
      <tr>
        <th>4</th>
        <td>1.0</td>
        <td>0.858583</td>
        <td>-0.468189</td>
        <td>0.656293</td>
        <td>0.970217</td>
        <td>0.566793</td>
        <td>0.037980</td>
        <td>-0.867851</td>
        <td>-0.055172</td>
        <td>-0.123488</td>
        <td>-0.594408</td>
      </tr>
      <tr>
        <th>...</th>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
      </tr>
      <tr>
        <th>2117</th>
        <td>1.0</td>
        <td>-0.325774</td>
        <td>-0.298574</td>
        <td>-0.984911</td>
        <td>-0.660747</td>
        <td>-0.894813</td>
        <td>0.971714</td>
        <td>-0.263716</td>
        <td>0.386797</td>
        <td>0.177803</td>
        <td>0.546942</td>
      </tr>
      <tr>
        <th>2118</th>
        <td>1.0</td>
        <td>0.631497</td>
        <td>-0.209968</td>
        <td>-0.060872</td>
        <td>-0.981552</td>
        <td>0.319807</td>
        <td>-0.552832</td>
        <td>-0.256842</td>
        <td>0.059649</td>
        <td>-0.120317</td>
        <td>0.194922</td>
      </tr>
      <tr>
        <th>2119</th>
        <td>1.0</td>
        <td>0.918874</td>
        <td>0.502420</td>
        <td>0.759211</td>
        <td>0.143963</td>
        <td>0.851615</td>
        <td>0.530987</td>
        <td>0.295923</td>
        <td>-0.576709</td>
        <td>-0.472256</td>
        <td>-0.470885</td>
      </tr>
      <tr>
        <th>2120</th>
        <td>1.0</td>
        <td>-0.505041</td>
        <td>-0.592865</td>
        <td>0.458442</td>
        <td>0.022174</td>
        <td>-0.396257</td>
        <td>0.430562</td>
        <td>0.394588</td>
        <td>0.286274</td>
        <td>0.493732</td>
        <td>-0.581601</td>
      </tr>
      <tr>
        <th>2121</th>
        <td>0.0</td>
        <td>0.304829</td>
        <td>0.028269</td>
        <td>0.903502</td>
        <td>-0.400436</td>
        <td>-0.932546</td>
        <td>-0.409703</td>
        <td>0.272170</td>
        <td>0.739227</td>
        <td>-0.744509</td>
        <td>-0.398449</td>
      </tr>
      </tbody>
    </table>
    </div>
    </div>

Here, 'Y' should be a categorical feature and it must be one of the columns in the data. By default, the ratios between different categories are the same. If you want to specify the ratios, you could use the `spark_sample_fractions` parameter, as shown below:

.. jupyter-input::

    exp.data_loader(data="./myfile.parquet", spark=True, spark_sample_size=10000,
                    spark_sample_by_feature='Y', spark_sample_fractions={0.0: 1, 1.0: 5},
                    spark_random_state=0)

.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
      <tr style="text-align: right;">
        <th></th>
        <th>Y</th>
        <th>X0</th>
        <th>X1</th>
        <th>X2</th>
        <th>X3</th>
        <th>X4</th>
        <th>X5</th>
        <th>X6</th>
        <th>X7</th>
        <th>X8</th>
        <th>X9</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th>0</th>
        <td>0.0</td>
        <td>-0.162726</td>
        <td>-0.989380</td>
        <td>-0.977290</td>
        <td>0.022444</td>
        <td>-0.833418</td>
        <td>-0.897849</td>
        <td>0.931033</td>
        <td>0.718005</td>
        <td>-0.695946</td>
        <td>-0.998672</td>
      </tr>
      <tr>
        <th>1</th>
        <td>1.0</td>
        <td>0.883336</td>
        <td>-0.443349</td>
        <td>-0.628205</td>
        <td>0.383016</td>
        <td>-0.782193</td>
        <td>-0.470701</td>
        <td>0.950189</td>
        <td>0.278926</td>
        <td>0.041356</td>
        <td>-0.204163</td>
      </tr>
      <tr>
        <th>2</th>
        <td>1.0</td>
        <td>0.549002</td>
        <td>-0.718085</td>
        <td>0.934676</td>
        <td>0.722246</td>
        <td>0.235314</td>
        <td>-0.914188</td>
        <td>0.401711</td>
        <td>0.826569</td>
        <td>0.049154</td>
        <td>-0.291550</td>
      </tr>
      <tr>
        <th>3</th>
        <td>0.0</td>
        <td>-0.759445</td>
        <td>0.509802</td>
        <td>0.770044</td>
        <td>-0.799497</td>
        <td>0.517969</td>
        <td>-0.965879</td>
        <td>0.934110</td>
        <td>0.230116</td>
        <td>0.104878</td>
        <td>-0.408100</td>
      </tr>
      <tr>
        <th>4</th>
        <td>1.0</td>
        <td>0.858583</td>
        <td>-0.468189</td>
        <td>0.656293</td>
        <td>0.970217</td>
        <td>0.566793</td>
        <td>0.037980</td>
        <td>-0.867851</td>
        <td>-0.055172</td>
        <td>-0.123488</td>
        <td>-0.594408</td>
      </tr>
      <tr>
        <th>...</th>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
        <td>...</td>
      </tr>
      <tr>
        <th>2117</th>
        <td>1.0</td>
        <td>-0.325774</td>
        <td>-0.298574</td>
        <td>-0.984911</td>
        <td>-0.660747</td>
        <td>-0.894813</td>
        <td>0.971714</td>
        <td>-0.263716</td>
        <td>0.386797</td>
        <td>0.177803</td>
        <td>0.546942</td>
      </tr>
      <tr>
        <th>2118</th>
        <td>1.0</td>
        <td>0.631497</td>
        <td>-0.209968</td>
        <td>-0.060872</td>
        <td>-0.981552</td>
        <td>0.319807</td>
        <td>-0.552832</td>
        <td>-0.256842</td>
        <td>0.059649</td>
        <td>-0.120317</td>
        <td>0.194922</td>
      </tr>
      <tr>
        <th>2119</th>
        <td>1.0</td>
        <td>0.918874</td>
        <td>0.502420</td>
        <td>0.759211</td>
        <td>0.143963</td>
        <td>0.851615</td>
        <td>0.530987</td>
        <td>0.295923</td>
        <td>-0.576709</td>
        <td>-0.472256</td>
        <td>-0.470885</td>
      </tr>
      <tr>
        <th>2120</th>
        <td>1.0</td>
        <td>-0.505041</td>
        <td>-0.592865</td>
        <td>0.458442</td>
        <td>0.022174</td>
        <td>-0.396257</td>
        <td>0.430562</td>
        <td>0.394588</td>
        <td>0.286274</td>
        <td>0.493732</td>
        <td>-0.581601</td>
      </tr>
      <tr>
        <th>2121</th>
        <td>0.0</td>
        <td>0.304829</td>
        <td>0.028269</td>
        <td>0.903502</td>
        <td>-0.400436</td>
        <td>-0.932546</td>
        <td>-0.409703</td>
        <td>0.272170</td>
        <td>0.739227</td>
        <td>-0.744509</td>
        <td>-0.398449</td>
      </tr>
      </tbody>
    </table>
    </div>
    </div>

The `spark_sample_fractions` parameter is a dictionary, where the keys are the categories and the values are the ratios. In the above example, the ratio between category 0.0 and 1.0 is 1:5.



Examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The full example codes of this section can be found in the following link.

.. topic:: Example 1: Load built-in datasets

  * :ref:`sphx_glr_auto_examples_0_data_plot_0_data_loader_builtin.py`

.. topic:: Example 2: Load data from pandas DataFrame

  * :ref:`sphx_glr_auto_examples_0_data_plot_0_data_loader_dataframe.py`

.. topic:: Example 3: Load data from spark DataFrame

  * :ref:`sphx_glr_auto_examples_0_data_plot_0_data_loader_spark.py`
